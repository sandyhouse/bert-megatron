{ // block 0
    var input_ids : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var segment_ids : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var input_mask : LOD_TENSOR.shape(-1, 1, 1, -1).dtype(FP32).stop_gradient(True)
    var masked_lm_positions : LOD_TENSOR.shape(-1,).dtype(INT64).stop_gradient(True)
    var masked_lm_labels : LOD_TENSOR.shape(-1, 1).dtype(INT64).stop_gradient(True)
    var next_sentence_labels : LOD_TENSOR.shape(-1, 1).dtype(INT64).stop_gradient(True)
    var masked_lm_scale : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist trainable param embedding_1.w_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_0.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_25.w_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_25.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param bert_lm_prediction_head_0.w_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist trainable param bert_lm_prediction_head_0.b_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var cumsum_0.tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    persist trainable param embedding_2.w_0 : LOD_TENSOR.shape(7633, 768).dtype(FP32).stop_gradient(False)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 1).dtype(INT64).stop_gradient(False)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1).dtype(INT64).stop_gradient(False)
    var shard_index_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 1).dtype(INT64).stop_gradient(False)
    var squeeze_0.tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(False)
    var squeeze_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 1).dtype(INT64).stop_gradient(False)
    var emb_rank_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_26.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_26.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_1 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_2 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_0.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_27.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_27.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_28.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_28.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_4 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_5 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_6 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_1.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_6 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_29.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_29.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_7 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_1.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_14.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_30.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_30.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_15.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_8 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_16.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_9 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_17.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_10 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_2.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_2.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_18.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_4 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_31.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_31.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_19.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_11 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_2.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_20.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_5 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_32.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_32.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_21.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_12 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_22.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_13 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_23.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_14 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_3.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_3.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_24.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_6 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_33.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_33.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_25.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_15 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_26.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_7 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_34.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_34.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_27.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_16 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_28.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_17 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_29.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_18 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_4.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_15 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_4.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_30.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_8 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_35.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_35.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_31.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_19 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_32.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_9 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_36.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_36.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_33.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_20 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_34.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_21 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_35.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_22 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_5.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_5.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_36.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_10 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_37.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_37.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_37.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_23 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_5.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_38.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_11 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_38.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_38.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_39.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_24 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_40.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_25 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_41.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_26 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_6.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_6.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_42.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_12 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_39.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_39.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_43.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_27 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_6.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_44.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_13 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_40.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_40.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_45.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_28 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_46.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_29 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_47.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_30 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_7.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_7.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_48.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_14 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_41.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_41.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_49.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_31 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_50.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_15 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_42.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_42.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_51.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_32 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_52.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_33 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_53.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_34 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_8.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_8.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_54.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_16 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_28 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_43.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_43.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_55.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_35 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_8.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_56.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_17 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_44.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_44.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_57.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_36 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_58.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_37 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_59.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_38 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_9.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_9.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_9.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_60.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_18 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_45.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_45.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_61.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_39 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_9.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_62.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_19 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_46.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_46.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_63.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_40 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_64.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_41 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_65.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_42 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_10.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_10.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_10.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_66.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_20 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_47.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_47.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_67.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_43 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_10.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_68.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_21 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_48.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_48.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_69.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_44 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_70.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_45 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    persist trainable param linear_71.w_0 : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_46 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_11.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_11.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_11.tmp_0 : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_0 : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_1 : LOD_TENSOR.shape(0, -1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_0 : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    persist trainable param linear_72.w_0 : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_22 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_37 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_49.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_49.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_73.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_47 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_74.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_23 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_50.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_50.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2_slice_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_1 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var tanh_1.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var gather_0.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_76.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var gelu_12.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_12.tmp_0 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var linear_77.tmp_0 : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_0 : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var embedding_2.w_0@GRAD : LOD_TENSOR.shape(7633, 768).dtype(FP32).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    var embedding_1.w_0@GRAD : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    var emb_rank_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_1@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_0.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_3@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_4@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_6@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_7@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_14.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var gelu_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_15.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_8@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_16.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_17.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_10@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_18.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_4@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_11@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_20.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_5@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_12@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_23.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_13@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_24.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_25.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_15@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_26.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var gelu_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_7@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_27.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_16@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_28.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_18@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_8@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_21.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_31.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_29.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_19@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_32.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_41@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_48.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_3@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_45@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_19@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_46@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_9@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_62.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_12@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_0.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_33.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_73.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_22@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var sum_0.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var linear_64.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_17@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_38@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_53.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_77.tmp_0@GRAD : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var gelu_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_33@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_37.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_30.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var linear_35.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var gelu_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_9@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_34.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_14@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_76.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_47@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_42@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_42.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_23@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_29@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_56.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_10@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_66.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_43.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_65.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_34@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_26@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_51.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var gelu_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_21@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tanh_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var gather_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var bert_lm_prediction_head_0.w_0@GRAD : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_22@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_36.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var linear_61.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_0.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_6@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_1@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_22.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_43@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var softmax_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var bert_lm_prediction_head_0.b_0@GRAD : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    var gelu_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_32@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var matmul_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var gelu_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_37@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_24@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_11@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_17@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_16@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_49.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_5@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var matmul_v2_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var gelu_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2_slice_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_74.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_69.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_72.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_57.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var gelu_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_70.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_44@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_55.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_21@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_68.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_15@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_67.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_27@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var softmax_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_40@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var linear_63.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_38.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_39@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_18@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_60.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var softmax_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_23@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_59.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_58.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_35@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_54.w_0@GRAD : LOD_TENSOR.shape(192, 768).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var gelu_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_52.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_71.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_50.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_31@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_14@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_45.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_30@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_47.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var linear_46.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_28@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_13@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_44.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var softmax_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, -1).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_25@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var gelu_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_row_rank_0.tmp_20@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_41.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_40.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_39.w_0@GRAD : LOD_TENSOR.shape(768, 192).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_36@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3, 64).dtype(FP32).stop_gradient(False)
    var linear_19.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 3, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var fc_by_col_rank_0.tmp_20@GRAD : LOD_TENSOR.shape(-1, -1, 192).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_moment1_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_moment2_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_moment1_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_moment2_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_moment1_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_moment2_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var linear_0.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var linear_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_moment1_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_moment2_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var tmp_42 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)

    {Out=['input_ids']} = c_broadcast(inputs={X=['input_ids']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, root = 0, use_calc_stream = True)
    {Out=['segment_ids']} = c_broadcast(inputs={X=['segment_ids']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, root = 0, use_calc_stream = True)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['input_ids']}, dtype = 3, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 1.0)
    {Out=['cumsum_0.tmp_0']} = cumsum(inputs={X=['full_like_0.tmp_0']}, axis = 1, exclusive = False, flatten = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], reverse = False)
    {Out=['tmp_0']} = elementwise_sub(inputs={X=['cumsum_0.tmp_0'], Y=['full_like_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['input_ids']}, axes = [-1], op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['shard_index_0.tmp_0']} = shard_index(inputs={X=['unsqueeze2_0.tmp_0']}, ignore_value = 7632, index_num = 30528, nshards = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shard_id = 0)
    {Out=['squeeze_0.tmp_0'], XShape=['squeeze_0.tmp_1']} = squeeze2(inputs={X=['shard_index_0.tmp_0']}, axes = [-1], op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['emb_rank_0.tmp_0']} = lookup_table_v2(inputs={Ids=['squeeze_0.tmp_0'], W=['embedding_2.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = 7632, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['emb_rank_0.tmp_0']} = c_allreduce_sum(inputs={X=['emb_rank_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['embedding_3.tmp_0']} = lookup_table_v2(inputs={Ids=['tmp_0'], W=['embedding_0.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_4.tmp_0']} = lookup_table_v2(inputs={Ids=['segment_ids'], W=['embedding_1.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['tmp_1']} = elementwise_add(inputs={X=['emb_rank_0.tmp_0'], Y=['embedding_3.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_2']} = elementwise_add(inputs={X=['tmp_1'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_26.tmp_0'], Variance=['layer_norm_26.tmp_1'], Y=['layer_norm_26.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_0.b_0'], Scale=['layer_norm_0.w_0'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_0']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_3.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['reshape2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_1']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_4.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_2']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_5.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['reshape2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_0.tmp_0']} = matmul(inputs={X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_3']} = elementwise_add(inputs={X=['matmul_0.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0']} = softmax(inputs={X=['tmp_3']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_v2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_0']} = matmul(inputs={X=['reshape2_3.tmp_0'], Y=['linear_6.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_0']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_4']} = elementwise_add(inputs={X=['layer_norm_26.tmp_2'], Y=['fc_by_row_rank_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_27.tmp_0'], Variance=['layer_norm_27.tmp_1'], Y=['layer_norm_27.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_3']} = matmul(inputs={X=['layer_norm_27.tmp_2'], Y=['linear_7.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_0.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_3']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_1']} = matmul(inputs={X=['gelu_0.tmp_0'], Y=['linear_8.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_1']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_5']} = elementwise_add(inputs={X=['layer_norm_27.tmp_2'], Y=['fc_by_row_rank_0.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_28.tmp_0'], Variance=['layer_norm_28.tmp_1'], Y=['layer_norm_28.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_4']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_9.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_4']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['reshape2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_5']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_10.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_6']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_11.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_5']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['reshape2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_6']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['reshape2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_1.tmp_0']} = matmul(inputs={X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_6']} = elementwise_add(inputs={X=['matmul_1.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_1.tmp_0']} = softmax(inputs={X=['tmp_6']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_2']} = matmul(inputs={X=['reshape2_7.tmp_0'], Y=['linear_12.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_2']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_2']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_7']} = elementwise_add(inputs={X=['layer_norm_28.tmp_2'], Y=['fc_by_row_rank_0.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_29.tmp_0'], Variance=['layer_norm_29.tmp_1'], Y=['layer_norm_29.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_7']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_7']} = matmul(inputs={X=['layer_norm_29.tmp_2'], Y=['linear_13.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_1.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_7']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_3']} = matmul(inputs={X=['gelu_1.tmp_0'], Y=['linear_14.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_3']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_8']} = elementwise_add(inputs={X=['layer_norm_29.tmp_2'], Y=['fc_by_row_rank_0.tmp_3']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_30.tmp_0'], Variance=['layer_norm_30.tmp_1'], Y=['layer_norm_30.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_8']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_15.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_8']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['reshape2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_9']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_16.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_10']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_17.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_9']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['reshape2_9.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_10']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['reshape2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_2.tmp_0']} = matmul(inputs={X=['transpose_8.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_9']} = elementwise_add(inputs={X=['matmul_2.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_2.tmp_0']} = softmax(inputs={X=['tmp_9']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['matmul_v2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_4']} = matmul(inputs={X=['reshape2_11.tmp_0'], Y=['linear_18.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_4']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_4']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_10']} = elementwise_add(inputs={X=['layer_norm_30.tmp_2'], Y=['fc_by_row_rank_0.tmp_4']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_31.tmp_0'], Variance=['layer_norm_31.tmp_1'], Y=['layer_norm_31.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_11']} = matmul(inputs={X=['layer_norm_31.tmp_2'], Y=['linear_19.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_2.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_11']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_5']} = matmul(inputs={X=['gelu_2.tmp_0'], Y=['linear_20.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_5']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_5']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_11']} = elementwise_add(inputs={X=['layer_norm_31.tmp_2'], Y=['fc_by_row_rank_0.tmp_5']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_32.tmp_0'], Variance=['layer_norm_32.tmp_1'], Y=['layer_norm_32.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_11']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_12']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_21.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_12']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['reshape2_12.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_13']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_22.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_14']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_23.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_13.tmp_0'], XShape=['reshape2_13.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_13']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['reshape2_13.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_14.tmp_0'], XShape=['reshape2_14.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_14']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['reshape2_14.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_3.tmp_0']} = matmul(inputs={X=['transpose_12.tmp_0'], Y=['transpose_13.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_12']} = elementwise_add(inputs={X=['matmul_3.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_3.tmp_0']} = softmax(inputs={X=['tmp_12']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_15.tmp_0'], XShape=['reshape2_15.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_6']} = matmul(inputs={X=['reshape2_15.tmp_0'], Y=['linear_24.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_6']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_6']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_13']} = elementwise_add(inputs={X=['layer_norm_32.tmp_2'], Y=['fc_by_row_rank_0.tmp_6']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_33.tmp_0'], Variance=['layer_norm_33.tmp_1'], Y=['layer_norm_33.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_13']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_15']} = matmul(inputs={X=['layer_norm_33.tmp_2'], Y=['linear_25.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_3.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_15']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_7']} = matmul(inputs={X=['gelu_3.tmp_0'], Y=['linear_26.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_7']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_7']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_14']} = elementwise_add(inputs={X=['layer_norm_33.tmp_2'], Y=['fc_by_row_rank_0.tmp_7']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_34.tmp_0'], Variance=['layer_norm_34.tmp_1'], Y=['layer_norm_34.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_14']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_16']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_27.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_16.tmp_0'], XShape=['reshape2_16.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_16']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['reshape2_16.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_17']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_28.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_18']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_29.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_17.tmp_0'], XShape=['reshape2_17.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_17']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['reshape2_17.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_18.tmp_0'], XShape=['reshape2_18.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_18']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['reshape2_18.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_4.tmp_0']} = matmul(inputs={X=['transpose_16.tmp_0'], Y=['transpose_17.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_15']} = elementwise_add(inputs={X=['matmul_4.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_4.tmp_0']} = softmax(inputs={X=['tmp_15']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_19.tmp_0'], XShape=['reshape2_19.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_8']} = matmul(inputs={X=['reshape2_19.tmp_0'], Y=['linear_30.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_8']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_8']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_16']} = elementwise_add(inputs={X=['layer_norm_34.tmp_2'], Y=['fc_by_row_rank_0.tmp_8']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_35.tmp_0'], Variance=['layer_norm_35.tmp_1'], Y=['layer_norm_35.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_16']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_19']} = matmul(inputs={X=['layer_norm_35.tmp_2'], Y=['linear_31.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_4.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_19']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_9']} = matmul(inputs={X=['gelu_4.tmp_0'], Y=['linear_32.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_9']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_9']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_17']} = elementwise_add(inputs={X=['layer_norm_35.tmp_2'], Y=['fc_by_row_rank_0.tmp_9']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_36.tmp_0'], Variance=['layer_norm_36.tmp_1'], Y=['layer_norm_36.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_17']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_20']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_33.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_20.tmp_0'], XShape=['reshape2_20.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_20']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_20.tmp_0'], XShape=['transpose_20.tmp_1']} = transpose2(inputs={X=['reshape2_20.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_21']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_34.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_22']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_35.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_21.tmp_0'], XShape=['reshape2_21.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_21']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_21.tmp_0'], XShape=['transpose_21.tmp_1']} = transpose2(inputs={X=['reshape2_21.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_22.tmp_0'], XShape=['reshape2_22.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_22']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_22.tmp_0'], XShape=['transpose_22.tmp_1']} = transpose2(inputs={X=['reshape2_22.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_5.tmp_0']} = matmul(inputs={X=['transpose_20.tmp_0'], Y=['transpose_21.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_18']} = elementwise_add(inputs={X=['matmul_5.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_5.tmp_0']} = softmax(inputs={X=['tmp_18']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_23.tmp_0'], XShape=['transpose_23.tmp_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_23.tmp_0'], XShape=['reshape2_23.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_10']} = matmul(inputs={X=['reshape2_23.tmp_0'], Y=['linear_36.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_10']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_10']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_19']} = elementwise_add(inputs={X=['layer_norm_36.tmp_2'], Y=['fc_by_row_rank_0.tmp_10']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_37.tmp_0'], Variance=['layer_norm_37.tmp_1'], Y=['layer_norm_37.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_19']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_23']} = matmul(inputs={X=['layer_norm_37.tmp_2'], Y=['linear_37.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_5.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_23']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_11']} = matmul(inputs={X=['gelu_5.tmp_0'], Y=['linear_38.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_11']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_11']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_20']} = elementwise_add(inputs={X=['layer_norm_37.tmp_2'], Y=['fc_by_row_rank_0.tmp_11']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_38.tmp_0'], Variance=['layer_norm_38.tmp_1'], Y=['layer_norm_38.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_20']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_24']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_39.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_24.tmp_0'], XShape=['reshape2_24.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_24']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_24.tmp_0'], XShape=['transpose_24.tmp_1']} = transpose2(inputs={X=['reshape2_24.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_25']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_40.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_26']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_41.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_25.tmp_0'], XShape=['reshape2_25.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_25']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_25.tmp_0'], XShape=['transpose_25.tmp_1']} = transpose2(inputs={X=['reshape2_25.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_26.tmp_0'], XShape=['reshape2_26.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_26']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_26.tmp_0'], XShape=['transpose_26.tmp_1']} = transpose2(inputs={X=['reshape2_26.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_6.tmp_0']} = matmul(inputs={X=['transpose_24.tmp_0'], Y=['transpose_25.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_21']} = elementwise_add(inputs={X=['matmul_6.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_6.tmp_0']} = softmax(inputs={X=['tmp_21']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_27.tmp_0'], XShape=['transpose_27.tmp_1']} = transpose2(inputs={X=['matmul_v2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_27.tmp_0'], XShape=['reshape2_27.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_27.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_12']} = matmul(inputs={X=['reshape2_27.tmp_0'], Y=['linear_42.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_12']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_12']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_22']} = elementwise_add(inputs={X=['layer_norm_38.tmp_2'], Y=['fc_by_row_rank_0.tmp_12']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_39.tmp_0'], Variance=['layer_norm_39.tmp_1'], Y=['layer_norm_39.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_22']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_27']} = matmul(inputs={X=['layer_norm_39.tmp_2'], Y=['linear_43.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_6.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_27']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_13']} = matmul(inputs={X=['gelu_6.tmp_0'], Y=['linear_44.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_13']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_13']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_23']} = elementwise_add(inputs={X=['layer_norm_39.tmp_2'], Y=['fc_by_row_rank_0.tmp_13']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_40.tmp_0'], Variance=['layer_norm_40.tmp_1'], Y=['layer_norm_40.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_23']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_28']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_45.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_28.tmp_0'], XShape=['reshape2_28.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_28']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_28.tmp_0'], XShape=['transpose_28.tmp_1']} = transpose2(inputs={X=['reshape2_28.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_29']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_46.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_30']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_47.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_29.tmp_0'], XShape=['reshape2_29.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_29']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_29.tmp_0'], XShape=['transpose_29.tmp_1']} = transpose2(inputs={X=['reshape2_29.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_30.tmp_0'], XShape=['reshape2_30.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_30']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_30.tmp_0'], XShape=['transpose_30.tmp_1']} = transpose2(inputs={X=['reshape2_30.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_7.tmp_0']} = matmul(inputs={X=['transpose_28.tmp_0'], Y=['transpose_29.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_24']} = elementwise_add(inputs={X=['matmul_7.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_7.tmp_0']} = softmax(inputs={X=['tmp_24']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_31.tmp_0'], XShape=['transpose_31.tmp_1']} = transpose2(inputs={X=['matmul_v2_7.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_31.tmp_0'], XShape=['reshape2_31.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_31.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_14']} = matmul(inputs={X=['reshape2_31.tmp_0'], Y=['linear_48.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_14']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_14']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_25']} = elementwise_add(inputs={X=['layer_norm_40.tmp_2'], Y=['fc_by_row_rank_0.tmp_14']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_41.tmp_0'], Variance=['layer_norm_41.tmp_1'], Y=['layer_norm_41.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_25']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_31']} = matmul(inputs={X=['layer_norm_41.tmp_2'], Y=['linear_49.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_7.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_31']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_15']} = matmul(inputs={X=['gelu_7.tmp_0'], Y=['linear_50.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_15']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_15']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_26']} = elementwise_add(inputs={X=['layer_norm_41.tmp_2'], Y=['fc_by_row_rank_0.tmp_15']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_42.tmp_0'], Variance=['layer_norm_42.tmp_1'], Y=['layer_norm_42.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_26']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_32']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_51.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_32.tmp_0'], XShape=['reshape2_32.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_32']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_32.tmp_0'], XShape=['transpose_32.tmp_1']} = transpose2(inputs={X=['reshape2_32.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_33']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_52.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_34']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_53.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_33.tmp_0'], XShape=['reshape2_33.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_33']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_33.tmp_0'], XShape=['transpose_33.tmp_1']} = transpose2(inputs={X=['reshape2_33.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_34.tmp_0'], XShape=['reshape2_34.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_34']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_34.tmp_0'], XShape=['transpose_34.tmp_1']} = transpose2(inputs={X=['reshape2_34.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_8.tmp_0']} = matmul(inputs={X=['transpose_32.tmp_0'], Y=['transpose_33.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_27']} = elementwise_add(inputs={X=['matmul_8.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_8.tmp_0']} = softmax(inputs={X=['tmp_27']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_35.tmp_0'], XShape=['transpose_35.tmp_1']} = transpose2(inputs={X=['matmul_v2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_35.tmp_0'], XShape=['reshape2_35.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_35.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_16']} = matmul(inputs={X=['reshape2_35.tmp_0'], Y=['linear_54.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_16']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_16']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_28']} = elementwise_add(inputs={X=['layer_norm_42.tmp_2'], Y=['fc_by_row_rank_0.tmp_16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_43.tmp_0'], Variance=['layer_norm_43.tmp_1'], Y=['layer_norm_43.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_28']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_35']} = matmul(inputs={X=['layer_norm_43.tmp_2'], Y=['linear_55.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_8.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_35']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_17']} = matmul(inputs={X=['gelu_8.tmp_0'], Y=['linear_56.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_17']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_17']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_29']} = elementwise_add(inputs={X=['layer_norm_43.tmp_2'], Y=['fc_by_row_rank_0.tmp_17']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_44.tmp_0'], Variance=['layer_norm_44.tmp_1'], Y=['layer_norm_44.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_29']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_36']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_57.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_36.tmp_0'], XShape=['reshape2_36.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_36']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_36.tmp_0'], XShape=['transpose_36.tmp_1']} = transpose2(inputs={X=['reshape2_36.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_37']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_58.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_38']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_59.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_37.tmp_0'], XShape=['reshape2_37.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_37']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_37.tmp_0'], XShape=['transpose_37.tmp_1']} = transpose2(inputs={X=['reshape2_37.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_38.tmp_0'], XShape=['reshape2_38.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_38']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_38.tmp_0'], XShape=['transpose_38.tmp_1']} = transpose2(inputs={X=['reshape2_38.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_9.tmp_0']} = matmul(inputs={X=['transpose_36.tmp_0'], Y=['transpose_37.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_30']} = elementwise_add(inputs={X=['matmul_9.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_9.tmp_0']} = softmax(inputs={X=['tmp_30']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_9.tmp_0']} = matmul_v2(inputs={X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_39.tmp_0'], XShape=['transpose_39.tmp_1']} = transpose2(inputs={X=['matmul_v2_9.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_39.tmp_0'], XShape=['reshape2_39.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_39.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_18']} = matmul(inputs={X=['reshape2_39.tmp_0'], Y=['linear_60.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_18']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_18']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_31']} = elementwise_add(inputs={X=['layer_norm_44.tmp_2'], Y=['fc_by_row_rank_0.tmp_18']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_45.tmp_0'], Variance=['layer_norm_45.tmp_1'], Y=['layer_norm_45.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_31']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_39']} = matmul(inputs={X=['layer_norm_45.tmp_2'], Y=['linear_61.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_9.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_39']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_19']} = matmul(inputs={X=['gelu_9.tmp_0'], Y=['linear_62.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_19']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_19']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_32']} = elementwise_add(inputs={X=['layer_norm_45.tmp_2'], Y=['fc_by_row_rank_0.tmp_19']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_46.tmp_0'], Variance=['layer_norm_46.tmp_1'], Y=['layer_norm_46.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_32']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_40']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_63.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_40.tmp_0'], XShape=['reshape2_40.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_40']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_40.tmp_0'], XShape=['transpose_40.tmp_1']} = transpose2(inputs={X=['reshape2_40.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_41']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_64.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_42']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_65.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_41.tmp_0'], XShape=['reshape2_41.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_41']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_41.tmp_0'], XShape=['transpose_41.tmp_1']} = transpose2(inputs={X=['reshape2_41.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_42.tmp_0'], XShape=['reshape2_42.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_42']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_42.tmp_0'], XShape=['transpose_42.tmp_1']} = transpose2(inputs={X=['reshape2_42.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_10.tmp_0']} = matmul(inputs={X=['transpose_40.tmp_0'], Y=['transpose_41.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_33']} = elementwise_add(inputs={X=['matmul_10.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_10.tmp_0']} = softmax(inputs={X=['tmp_33']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_10.tmp_0']} = matmul_v2(inputs={X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_43.tmp_0'], XShape=['transpose_43.tmp_1']} = transpose2(inputs={X=['matmul_v2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_43.tmp_0'], XShape=['reshape2_43.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_43.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_20']} = matmul(inputs={X=['reshape2_43.tmp_0'], Y=['linear_66.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_20']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_20']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_34']} = elementwise_add(inputs={X=['layer_norm_46.tmp_2'], Y=['fc_by_row_rank_0.tmp_20']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_47.tmp_0'], Variance=['layer_norm_47.tmp_1'], Y=['layer_norm_47.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_34']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_43']} = matmul(inputs={X=['layer_norm_47.tmp_2'], Y=['linear_67.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_10.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_43']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_21']} = matmul(inputs={X=['gelu_10.tmp_0'], Y=['linear_68.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_21']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_21']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_35']} = elementwise_add(inputs={X=['layer_norm_47.tmp_2'], Y=['fc_by_row_rank_0.tmp_21']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_48.tmp_0'], Variance=['layer_norm_48.tmp_1'], Y=['layer_norm_48.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_35']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_44']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_69.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_44.tmp_0'], XShape=['reshape2_44.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_44']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_44.tmp_0'], XShape=['transpose_44.tmp_1']} = transpose2(inputs={X=['reshape2_44.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_45']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_70.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_col_rank_0.tmp_46']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_71.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_45.tmp_0'], XShape=['reshape2_45.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_45']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_45.tmp_0'], XShape=['transpose_45.tmp_1']} = transpose2(inputs={X=['reshape2_45.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_46.tmp_0'], XShape=['reshape2_46.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_by_col_rank_0.tmp_46']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {Out=['transpose_46.tmp_0'], XShape=['transpose_46.tmp_1']} = transpose2(inputs={X=['reshape2_46.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_11.tmp_0']} = matmul(inputs={X=['transpose_44.tmp_0'], Y=['transpose_45.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_36']} = elementwise_add(inputs={X=['matmul_11.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_11.tmp_0']} = softmax(inputs={X=['tmp_36']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_11.tmp_0']} = matmul_v2(inputs={X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_47.tmp_0'], XShape=['transpose_47.tmp_1']} = transpose2(inputs={X=['matmul_v2_11.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_47.tmp_0'], XShape=['reshape2_47.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_47.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_22']} = matmul(inputs={X=['reshape2_47.tmp_0'], Y=['linear_72.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_22']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_22']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_37']} = elementwise_add(inputs={X=['layer_norm_48.tmp_2'], Y=['fc_by_row_rank_0.tmp_22']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_49.tmp_0'], Variance=['layer_norm_49.tmp_1'], Y=['layer_norm_49.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_37']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['fc_by_col_rank_0.tmp_47']} = matmul(inputs={X=['layer_norm_49.tmp_2'], Y=['linear_73.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_11.tmp_0']} = gelu(inputs={X=['fc_by_col_rank_0.tmp_47']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_by_row_rank_0.tmp_23']} = matmul(inputs={X=['gelu_11.tmp_0'], Y=['linear_74.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['fc_by_row_rank_0.tmp_23']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_23']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {Out=['tmp_38']} = elementwise_add(inputs={X=['layer_norm_49.tmp_2'], Y=['fc_by_row_rank_0.tmp_23']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_50.tmp_0'], Variance=['layer_norm_50.tmp_1'], Y=['layer_norm_50.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_38']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['layer_norm_50.tmp_2_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['layer_norm_50.tmp_2'], StartsTensor=[], StartsTensorList=[]}, axes = [1], decrease_axis = [1], ends = [1], infer_flags = [1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], starts = [0])
    {Out=['linear_75.tmp_0']} = matmul(inputs={X=['layer_norm_50.tmp_2_slice_0'], Y=['linear_0.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_75.tmp_1']} = elementwise_add(inputs={X=['linear_75.tmp_0'], Y=['linear_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tanh_1.tmp_0']} = tanh(inputs={X=['linear_75.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['reshape2_48.tmp_0'], XShape=['reshape2_48.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['layer_norm_50.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Out=['gather_0.tmp_0']} = gather(inputs={Axis=[], Index=['masked_lm_positions'], X=['reshape2_48.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], overwrite = False)
    {Out=['linear_76.tmp_0']} = matmul(inputs={X=['gather_0.tmp_0'], Y=['linear_1.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_12.tmp_0']} = gelu(inputs={X=['linear_76.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['matmul_v2_12.tmp_0']} = matmul_v2(inputs={X=['gelu_12.tmp_0'], Y=['bert_lm_prediction_head_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['tmp_39']} = elementwise_add(inputs={X=['matmul_v2_12.tmp_0'], Y=['bert_lm_prediction_head_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_77.tmp_0']} = matmul(inputs={X=['tanh_1.tmp_0'], Y=['linear_2.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['masked_lm_labels'], Logits=['tmp_39']}, axis = -1, ignore_index = -1, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False)
    {Out=['tmp_40']} = elementwise_div(inputs={X=['softmax_with_cross_entropy_0.tmp_1'], Y=['masked_lm_scale']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Loss=['softmax_with_cross_entropy_1.tmp_1'], Softmax=['softmax_with_cross_entropy_1.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['next_sentence_labels'], Logits=['linear_77.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False)
    {Out=['sum_0.tmp_0']} = reduce_sum(inputs={X=['tmp_40']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['softmax_with_cross_entropy_1.tmp_1']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Out=['tmp_41']} = elementwise_add(inputs={X=['sum_0.tmp_0'], Y=['mean_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 256, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_41@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_role = 257, shape = [1], value = 1.0)
    {X@GRAD=['sum_0.tmp_0@GRAD'], Y@GRAD=['mean_0.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_41@GRAD'], X=['sum_0.tmp_0'], Y=['mean_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['softmax_with_cross_entropy_1.tmp_1@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['softmax_with_cross_entropy_1.tmp_1']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True)
    {X@GRAD=['tmp_40@GRAD']} = reduce_sum_grad(inputs={Out@GRAD=['sum_0.tmp_0@GRAD'], X=['tmp_40']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Logits@GRAD=['linear_77.tmp_0@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['next_sentence_labels'], Loss@GRAD=['softmax_with_cross_entropy_1.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_1.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False)
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Y@GRAD=[]} = elementwise_div_grad(inputs={Out=['tmp_40'], Out@GRAD=['tmp_40@GRAD'], X=['softmax_with_cross_entropy_0.tmp_1'], Y=['masked_lm_scale']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Logits@GRAD=['tmp_39@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['masked_lm_labels'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -1, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False)
    {X@GRAD=['tanh_1.tmp_0@GRAD'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_77.tmp_0@GRAD'], X=['tanh_1.tmp_0'], Y=['linear_2.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['matmul_v2_12.tmp_0@GRAD'], Y@GRAD=['bert_lm_prediction_head_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['matmul_v2_12.tmp_0'], Y=['bert_lm_prediction_head_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['bert_lm_prediction_head_0.b_0', 'bert_lm_prediction_head_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_12.tmp_0@GRAD'], Y@GRAD=['bert_lm_prediction_head_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_12.tmp_0@GRAD'], X=['gelu_12.tmp_0'], Y=['bert_lm_prediction_head_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], trans_x = False, trans_y = False)
    {X@GRAD=['linear_76.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_12.tmp_0@GRAD'], X=['linear_76.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['gather_0.tmp_0@GRAD'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_76.tmp_0@GRAD'], X=['gather_0.tmp_0'], Y=['linear_1.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_48.tmp_0@GRAD']} = gather_grad(inputs={Axis=[], Index=['masked_lm_positions'], Out@GRAD=['gather_0.tmp_0@GRAD'], X=['reshape2_48.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], overwrite = False)
    {X@GRAD=['layer_norm_50.tmp_2@GRAD@RENAME@block0@0']} = reshape2_grad(inputs={Out@GRAD=['reshape2_48.tmp_0@GRAD'], XShape=['reshape2_48.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {X@GRAD=['linear_75.tmp_1@GRAD']} = tanh_grad(inputs={Out=['tanh_1.tmp_0'], Out@GRAD=['tanh_1.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['linear_75.tmp_0@GRAD'], Y@GRAD=['linear_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_75.tmp_1@GRAD'], X=['linear_75.tmp_0'], Y=['linear_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_0.b_0', 'linear_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_50.tmp_2_slice_0@GRAD'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_75.tmp_0@GRAD'], X=['layer_norm_50.tmp_2_slice_0'], Y=['linear_0.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Input@GRAD=['layer_norm_50.tmp_2@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['layer_norm_50.tmp_2'], Out@GRAD=['layer_norm_50.tmp_2_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [1], decrease_axis = [1], ends = [1], infer_flags = [1], op_device = , op_namescope = /, op_role = 1, op_role_var = [], starts = [0])
    {Out=['layer_norm_50.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_50.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_50.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_38@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_50.tmp_0'], Scale=[], Variance=['layer_norm_50.tmp_1'], X=['tmp_38'], Y@GRAD=['layer_norm_50.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_49.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_23@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_38@GRAD'], X=['layer_norm_49.tmp_2'], Y=['fc_by_row_rank_0.tmp_23']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_23@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_23@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_11.tmp_0@GRAD'], Y@GRAD=['linear_74.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_23@GRAD'], X=['gelu_11.tmp_0'], Y=['linear_74.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_47@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_11.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_47']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_49.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_73.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_47@GRAD'], X=['layer_norm_49.tmp_2'], Y=['linear_73.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_49.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_49.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_49.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_37@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_49.tmp_0'], Scale=[], Variance=['layer_norm_49.tmp_1'], X=['tmp_37'], Y@GRAD=['layer_norm_49.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_22@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['layer_norm_48.tmp_2'], Y=['fc_by_row_rank_0.tmp_22']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_22@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_22@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_47.tmp_0@GRAD'], Y@GRAD=['linear_72.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_22@GRAD'], X=['reshape2_47.tmp_0'], Y=['linear_72.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_47.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_47.tmp_0@GRAD'], XShape=['reshape2_47.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_47.tmp_0@GRAD'], XShape=['transpose_47.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_11.tmp_0@GRAD'], Y@GRAD=['transpose_46.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_11.tmp_0@GRAD'], X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_36@GRAD']} = softmax_grad(inputs={Out=['softmax_11.tmp_0'], Out@GRAD=['softmax_11.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_11.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_36@GRAD'], X=['matmul_11.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_44.tmp_0@GRAD'], Y@GRAD=['transpose_45.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_11.tmp_0@GRAD'], X=['transpose_44.tmp_0'], Y=['transpose_45.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_46.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_46.tmp_0@GRAD'], XShape=['transpose_46.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_46@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_46.tmp_0@GRAD'], XShape=['reshape2_46.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_45.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_45.tmp_0@GRAD'], XShape=['transpose_45.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_45@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_45.tmp_0@GRAD'], XShape=['reshape2_45.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_71.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_46@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_71.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_70.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_45@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_70.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_44.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_44.tmp_0@GRAD'], XShape=['transpose_44.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_44@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_44.tmp_0@GRAD'], XShape=['reshape2_44.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_69.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_44@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_69.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_48.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_48.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_35@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_48.tmp_0'], Scale=[], Variance=['layer_norm_48.tmp_1'], X=['tmp_35'], Y@GRAD=['layer_norm_48.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_47.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_21@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['layer_norm_47.tmp_2'], Y=['fc_by_row_rank_0.tmp_21']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_21@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_21@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_10.tmp_0@GRAD'], Y@GRAD=['linear_68.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_21@GRAD'], X=['gelu_10.tmp_0'], Y=['linear_68.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_43@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_10.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_43']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_47.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_67.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_43@GRAD'], X=['layer_norm_47.tmp_2'], Y=['linear_67.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_47.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_47.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_47.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_34@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_47.tmp_0'], Scale=[], Variance=['layer_norm_47.tmp_1'], X=['tmp_34'], Y@GRAD=['layer_norm_47.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_20@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['layer_norm_46.tmp_2'], Y=['fc_by_row_rank_0.tmp_20']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_20@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_20@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_43.tmp_0@GRAD'], Y@GRAD=['linear_66.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_20@GRAD'], X=['reshape2_43.tmp_0'], Y=['linear_66.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_43.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_43.tmp_0@GRAD'], XShape=['reshape2_43.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_43.tmp_0@GRAD'], XShape=['transpose_43.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_10.tmp_0@GRAD'], Y@GRAD=['transpose_42.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_10.tmp_0@GRAD'], X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_33@GRAD']} = softmax_grad(inputs={Out=['softmax_10.tmp_0'], Out@GRAD=['softmax_10.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_10.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_33@GRAD'], X=['matmul_10.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_40.tmp_0@GRAD'], Y@GRAD=['transpose_41.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_10.tmp_0@GRAD'], X=['transpose_40.tmp_0'], Y=['transpose_41.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_42.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_42.tmp_0@GRAD'], XShape=['transpose_42.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_42@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_42.tmp_0@GRAD'], XShape=['reshape2_42.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_41.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_41.tmp_0@GRAD'], XShape=['transpose_41.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_41@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_41.tmp_0@GRAD'], XShape=['reshape2_41.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_65.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_42@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_65.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_64.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_41@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_64.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_40.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_40.tmp_0@GRAD'], XShape=['transpose_40.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_40@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_40.tmp_0@GRAD'], XShape=['reshape2_40.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_63.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_40@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_63.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_46.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_46.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_32@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_46.tmp_0'], Scale=[], Variance=['layer_norm_46.tmp_1'], X=['tmp_32'], Y@GRAD=['layer_norm_46.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_45.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_19@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['layer_norm_45.tmp_2'], Y=['fc_by_row_rank_0.tmp_19']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_19@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_19@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_9.tmp_0@GRAD'], Y@GRAD=['linear_62.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_19@GRAD'], X=['gelu_9.tmp_0'], Y=['linear_62.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_39@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_9.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_39']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_45.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_61.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_39@GRAD'], X=['layer_norm_45.tmp_2'], Y=['linear_61.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_45.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_45.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_45.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_31@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_45.tmp_0'], Scale=[], Variance=['layer_norm_45.tmp_1'], X=['tmp_31'], Y@GRAD=['layer_norm_45.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_18@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['layer_norm_44.tmp_2'], Y=['fc_by_row_rank_0.tmp_18']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_18@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_18@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_39.tmp_0@GRAD'], Y@GRAD=['linear_60.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_18@GRAD'], X=['reshape2_39.tmp_0'], Y=['linear_60.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_39.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_39.tmp_0@GRAD'], XShape=['reshape2_39.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_39.tmp_0@GRAD'], XShape=['transpose_39.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_9.tmp_0@GRAD'], Y@GRAD=['transpose_38.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_9.tmp_0@GRAD'], X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_30@GRAD']} = softmax_grad(inputs={Out=['softmax_9.tmp_0'], Out@GRAD=['softmax_9.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_9.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['matmul_9.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_36.tmp_0@GRAD'], Y@GRAD=['transpose_37.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_9.tmp_0@GRAD'], X=['transpose_36.tmp_0'], Y=['transpose_37.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_38.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_38.tmp_0@GRAD'], XShape=['transpose_38.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_38@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_38.tmp_0@GRAD'], XShape=['reshape2_38.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_37.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_37.tmp_0@GRAD'], XShape=['transpose_37.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_37@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_37.tmp_0@GRAD'], XShape=['reshape2_37.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_59.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_38@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_59.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_58.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_37@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_58.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_36.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_36.tmp_0@GRAD'], XShape=['transpose_36.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_36@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_36.tmp_0@GRAD'], XShape=['reshape2_36.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_57.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_36@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_57.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_44.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_44.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_29@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_44.tmp_0'], Scale=[], Variance=['layer_norm_44.tmp_1'], X=['tmp_29'], Y@GRAD=['layer_norm_44.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_43.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_17@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_29@GRAD'], X=['layer_norm_43.tmp_2'], Y=['fc_by_row_rank_0.tmp_17']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_17@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_17@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_8.tmp_0@GRAD'], Y@GRAD=['linear_56.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_17@GRAD'], X=['gelu_8.tmp_0'], Y=['linear_56.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_35@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_8.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_35']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_43.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_55.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_35@GRAD'], X=['layer_norm_43.tmp_2'], Y=['linear_55.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_43.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_43.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_43.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_28@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_43.tmp_0'], Scale=[], Variance=['layer_norm_43.tmp_1'], X=['tmp_28'], Y@GRAD=['layer_norm_43.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['layer_norm_42.tmp_2'], Y=['fc_by_row_rank_0.tmp_16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_16@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_16@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_35.tmp_0@GRAD'], Y@GRAD=['linear_54.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_16@GRAD'], X=['reshape2_35.tmp_0'], Y=['linear_54.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_35.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_35.tmp_0@GRAD'], XShape=['reshape2_35.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_35.tmp_0@GRAD'], XShape=['transpose_35.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_8.tmp_0@GRAD'], Y@GRAD=['transpose_34.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_8.tmp_0@GRAD'], X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_27@GRAD']} = softmax_grad(inputs={Out=['softmax_8.tmp_0'], Out@GRAD=['softmax_8.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_8.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['matmul_8.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_32.tmp_0@GRAD'], Y@GRAD=['transpose_33.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_8.tmp_0@GRAD'], X=['transpose_32.tmp_0'], Y=['transpose_33.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_34.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_34.tmp_0@GRAD'], XShape=['transpose_34.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_34@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_34.tmp_0@GRAD'], XShape=['reshape2_34.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_33.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_33.tmp_0@GRAD'], XShape=['transpose_33.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_33@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_33.tmp_0@GRAD'], XShape=['reshape2_33.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_53.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_34@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_53.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_52.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_33@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_52.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_32.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_32.tmp_0@GRAD'], XShape=['transpose_32.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_32@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_32.tmp_0@GRAD'], XShape=['reshape2_32.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_51.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_32@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_51.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_42.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_42.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_26@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_42.tmp_0'], Scale=[], Variance=['layer_norm_42.tmp_1'], X=['tmp_26'], Y@GRAD=['layer_norm_42.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_41.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_15@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['layer_norm_41.tmp_2'], Y=['fc_by_row_rank_0.tmp_15']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_15@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_15@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_7.tmp_0@GRAD'], Y@GRAD=['linear_50.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_15@GRAD'], X=['gelu_7.tmp_0'], Y=['linear_50.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_31@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_7.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_31']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_41.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_49.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_31@GRAD'], X=['layer_norm_41.tmp_2'], Y=['linear_49.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_41.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_41.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_41.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_25@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_41.tmp_0'], Scale=[], Variance=['layer_norm_41.tmp_1'], X=['tmp_25'], Y@GRAD=['layer_norm_41.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_14@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_25@GRAD'], X=['layer_norm_40.tmp_2'], Y=['fc_by_row_rank_0.tmp_14']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_14@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_14@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_31.tmp_0@GRAD'], Y@GRAD=['linear_48.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_14@GRAD'], X=['reshape2_31.tmp_0'], Y=['linear_48.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_31.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_31.tmp_0@GRAD'], XShape=['reshape2_31.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_31.tmp_0@GRAD'], XShape=['transpose_31.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_7.tmp_0@GRAD'], Y@GRAD=['transpose_30.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_24@GRAD']} = softmax_grad(inputs={Out=['softmax_7.tmp_0'], Out@GRAD=['softmax_7.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_7.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_24@GRAD'], X=['matmul_7.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_28.tmp_0@GRAD'], Y@GRAD=['transpose_29.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_7.tmp_0@GRAD'], X=['transpose_28.tmp_0'], Y=['transpose_29.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_30.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_30.tmp_0@GRAD'], XShape=['transpose_30.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_30@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_30.tmp_0@GRAD'], XShape=['reshape2_30.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_29.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_29.tmp_0@GRAD'], XShape=['transpose_29.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_29@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_29.tmp_0@GRAD'], XShape=['reshape2_29.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_47.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_30@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_47.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_46.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_29@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_46.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_28.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_28.tmp_0@GRAD'], XShape=['transpose_28.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_28@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_28.tmp_0@GRAD'], XShape=['reshape2_28.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_45.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_28@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_45.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_40.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_40.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_23@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_40.tmp_0'], Scale=[], Variance=['layer_norm_40.tmp_1'], X=['tmp_23'], Y@GRAD=['layer_norm_40.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_39.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_13@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['layer_norm_39.tmp_2'], Y=['fc_by_row_rank_0.tmp_13']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_13@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_13@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_6.tmp_0@GRAD'], Y@GRAD=['linear_44.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_13@GRAD'], X=['gelu_6.tmp_0'], Y=['linear_44.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_27@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_6.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_27']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_39.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_43.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_27@GRAD'], X=['layer_norm_39.tmp_2'], Y=['linear_43.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_39.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_39.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_39.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_22@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_39.tmp_0'], Scale=[], Variance=['layer_norm_39.tmp_1'], X=['tmp_22'], Y@GRAD=['layer_norm_39.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_12@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['layer_norm_38.tmp_2'], Y=['fc_by_row_rank_0.tmp_12']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_12@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_12@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_27.tmp_0@GRAD'], Y@GRAD=['linear_42.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_12@GRAD'], X=['reshape2_27.tmp_0'], Y=['linear_42.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_27.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_27.tmp_0@GRAD'], XShape=['reshape2_27.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_27.tmp_0@GRAD'], XShape=['transpose_27.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_6.tmp_0@GRAD'], Y@GRAD=['transpose_26.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_21@GRAD']} = softmax_grad(inputs={Out=['softmax_6.tmp_0'], Out@GRAD=['softmax_6.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['matmul_6.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_24.tmp_0@GRAD'], Y@GRAD=['transpose_25.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_6.tmp_0@GRAD'], X=['transpose_24.tmp_0'], Y=['transpose_25.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_26.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_26.tmp_0@GRAD'], XShape=['transpose_26.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_26@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_26.tmp_0@GRAD'], XShape=['reshape2_26.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_25.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_25.tmp_0@GRAD'], XShape=['transpose_25.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_25@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_25.tmp_0@GRAD'], XShape=['reshape2_25.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_41.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_26@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_41.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_40.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_25@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_40.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_24.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_24.tmp_0@GRAD'], XShape=['transpose_24.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_24@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_24.tmp_0@GRAD'], XShape=['reshape2_24.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_39.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_24@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_39.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_38.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_38.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_20@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_38.tmp_0'], Scale=[], Variance=['layer_norm_38.tmp_1'], X=['tmp_20'], Y@GRAD=['layer_norm_38.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_37.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_11@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['layer_norm_37.tmp_2'], Y=['fc_by_row_rank_0.tmp_11']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_11@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_11@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_5.tmp_0@GRAD'], Y@GRAD=['linear_38.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_11@GRAD'], X=['gelu_5.tmp_0'], Y=['linear_38.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_23@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_5.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_23']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_37.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_37.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_23@GRAD'], X=['layer_norm_37.tmp_2'], Y=['linear_37.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_37.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_37.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_37.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_19@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_37.tmp_0'], Scale=[], Variance=['layer_norm_37.tmp_1'], X=['tmp_19'], Y@GRAD=['layer_norm_37.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_10@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_19@GRAD'], X=['layer_norm_36.tmp_2'], Y=['fc_by_row_rank_0.tmp_10']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_10@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_10@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_23.tmp_0@GRAD'], Y@GRAD=['linear_36.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_10@GRAD'], X=['reshape2_23.tmp_0'], Y=['linear_36.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_23.tmp_0@GRAD'], XShape=['reshape2_23.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_23.tmp_0@GRAD'], XShape=['transpose_23.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_5.tmp_0@GRAD'], Y@GRAD=['transpose_22.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_18@GRAD']} = softmax_grad(inputs={Out=['softmax_5.tmp_0'], Out@GRAD=['softmax_5.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_18@GRAD'], X=['matmul_5.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_20.tmp_0@GRAD'], Y@GRAD=['transpose_21.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_5.tmp_0@GRAD'], X=['transpose_20.tmp_0'], Y=['transpose_21.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_22.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_22.tmp_0@GRAD'], XShape=['transpose_22.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_22@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_22.tmp_0@GRAD'], XShape=['reshape2_22.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_21.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_21.tmp_0@GRAD'], XShape=['transpose_21.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_21@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_21.tmp_0@GRAD'], XShape=['reshape2_21.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_35.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_22@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_35.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_34.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_21@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_34.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_20.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_20.tmp_0@GRAD'], XShape=['transpose_20.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_20@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_20.tmp_0@GRAD'], XShape=['reshape2_20.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_33.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_20@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_33.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_36.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_36.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_17@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_36.tmp_0'], Scale=[], Variance=['layer_norm_36.tmp_1'], X=['tmp_17'], Y@GRAD=['layer_norm_36.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_35.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_9@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['layer_norm_35.tmp_2'], Y=['fc_by_row_rank_0.tmp_9']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_9@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_9@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_4.tmp_0@GRAD'], Y@GRAD=['linear_32.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_9@GRAD'], X=['gelu_4.tmp_0'], Y=['linear_32.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_19@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_4.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_19']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_35.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_31.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_19@GRAD'], X=['layer_norm_35.tmp_2'], Y=['linear_31.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_35.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_35.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_35.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_16@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_35.tmp_0'], Scale=[], Variance=['layer_norm_35.tmp_1'], X=['tmp_16'], Y@GRAD=['layer_norm_35.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_8@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_16@GRAD'], X=['layer_norm_34.tmp_2'], Y=['fc_by_row_rank_0.tmp_8']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_8@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_8@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_19.tmp_0@GRAD'], Y@GRAD=['linear_30.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_8@GRAD'], X=['reshape2_19.tmp_0'], Y=['linear_30.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_4.tmp_0@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_15@GRAD']} = softmax_grad(inputs={Out=['softmax_4.tmp_0'], Out@GRAD=['softmax_4.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['matmul_4.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_16.tmp_0@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_4.tmp_0@GRAD'], X=['transpose_16.tmp_0'], Y=['transpose_17.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_18.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_18@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_18.tmp_0@GRAD'], XShape=['reshape2_18.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_17@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_17.tmp_0@GRAD'], XShape=['reshape2_17.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_29.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_18@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_29.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_28.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_17@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_28.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_16@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_16.tmp_0@GRAD'], XShape=['reshape2_16.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_27.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_16@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_27.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_34.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_34.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_14@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_34.tmp_0'], Scale=[], Variance=['layer_norm_34.tmp_1'], X=['tmp_14'], Y@GRAD=['layer_norm_34.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_33.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_7@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_14@GRAD'], X=['layer_norm_33.tmp_2'], Y=['fc_by_row_rank_0.tmp_7']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_7@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_7@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_3.tmp_0@GRAD'], Y@GRAD=['linear_26.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_7@GRAD'], X=['gelu_3.tmp_0'], Y=['linear_26.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_15@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_3.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_15']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_33.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_25.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_15@GRAD'], X=['layer_norm_33.tmp_2'], Y=['linear_25.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_33.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_33.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_33.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_13@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_33.tmp_0'], Scale=[], Variance=['layer_norm_33.tmp_1'], X=['tmp_13'], Y@GRAD=['layer_norm_33.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_6@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['layer_norm_32.tmp_2'], Y=['fc_by_row_rank_0.tmp_6']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_6@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_6@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_15.tmp_0@GRAD'], Y@GRAD=['linear_24.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_6@GRAD'], X=['reshape2_15.tmp_0'], Y=['linear_24.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_3.tmp_0@GRAD'], Y@GRAD=['transpose_14.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_12@GRAD']} = softmax_grad(inputs={Out=['softmax_3.tmp_0'], Out@GRAD=['softmax_3.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['matmul_3.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_12.tmp_0@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_3.tmp_0@GRAD'], X=['transpose_12.tmp_0'], Y=['transpose_13.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_14.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_14@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_14.tmp_0@GRAD'], XShape=['reshape2_14.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_13.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_13@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_13.tmp_0@GRAD'], XShape=['reshape2_13.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_23.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_14@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_23.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_22.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_13@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_22.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_12@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_21.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_12@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_21.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_32.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_32.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_11@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_32.tmp_0'], Scale=[], Variance=['layer_norm_32.tmp_1'], X=['tmp_11'], Y@GRAD=['layer_norm_32.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_31.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_5@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_11@GRAD'], X=['layer_norm_31.tmp_2'], Y=['fc_by_row_rank_0.tmp_5']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_5@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_5@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_2.tmp_0@GRAD'], Y@GRAD=['linear_20.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_5@GRAD'], X=['gelu_2.tmp_0'], Y=['linear_20.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_11@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_2.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_11']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_31.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_19.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_11@GRAD'], X=['layer_norm_31.tmp_2'], Y=['linear_19.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_31.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_31.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_31.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_10@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_31.tmp_0'], Scale=[], Variance=['layer_norm_31.tmp_1'], X=['tmp_10'], Y@GRAD=['layer_norm_31.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_4@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['layer_norm_30.tmp_2'], Y=['fc_by_row_rank_0.tmp_4']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_4@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_4@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_11.tmp_0@GRAD'], Y@GRAD=['linear_18.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_4@GRAD'], X=['reshape2_11.tmp_0'], Y=['linear_18.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD'], Y@GRAD=['transpose_10.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_9@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_0'], Out@GRAD=['softmax_2.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['matmul_2.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_8.tmp_0@GRAD'], Y@GRAD=['transpose_9.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_2.tmp_0@GRAD'], X=['transpose_8.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_10@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_9@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_17.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_10@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_17.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_16.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_9@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_16.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_8@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_8.tmp_0@GRAD'], XShape=['reshape2_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_15.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_8@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_15.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_30.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_30.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_8@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_30.tmp_0'], Scale=[], Variance=['layer_norm_30.tmp_1'], X=['tmp_8'], Y@GRAD=['layer_norm_30.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_29.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_3@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['layer_norm_29.tmp_2'], Y=['fc_by_row_rank_0.tmp_3']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_3@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_3@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_1.tmp_0@GRAD'], Y@GRAD=['linear_14.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_3@GRAD'], X=['gelu_1.tmp_0'], Y=['linear_14.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_7@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_1.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_7']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_29.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_13.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_7@GRAD'], X=['layer_norm_29.tmp_2'], Y=['linear_13.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_29.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_29.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_29.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_7@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_29.tmp_0'], Scale=[], Variance=['layer_norm_29.tmp_1'], X=['tmp_7'], Y@GRAD=['layer_norm_29.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_2@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_7@GRAD'], X=['layer_norm_28.tmp_2'], Y=['fc_by_row_rank_0.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_2@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_2@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_7.tmp_0@GRAD'], Y@GRAD=['linear_12.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_2@GRAD'], X=['reshape2_7.tmp_0'], Y=['linear_12.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_6@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_0'], Out@GRAD=['softmax_1.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['matmul_1.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_4.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_1.tmp_0@GRAD'], X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_6@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_5@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_6@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_11.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_5@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_10.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_4@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_4@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_9.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_28.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_28.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_5@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_28.tmp_0'], Scale=[], Variance=['layer_norm_28.tmp_1'], X=['tmp_5'], Y@GRAD=['layer_norm_28.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_27.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['layer_norm_27.tmp_2'], Y=['fc_by_row_rank_0.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_1@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_1@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['gelu_0.tmp_0@GRAD'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_1@GRAD'], X=['gelu_0.tmp_0'], Y=['linear_8.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_3@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_0.tmp_0@GRAD'], X=['fc_by_col_rank_0.tmp_3']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_27.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_3@GRAD'], X=['layer_norm_27.tmp_2'], Y=['linear_7.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_27.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_27.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_27.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_4@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_27.tmp_0'], Scale=[], Variance=['layer_norm_27.tmp_1'], X=['tmp_4'], Y@GRAD=['layer_norm_27.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['fc_by_row_rank_0.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['layer_norm_26.tmp_2'], Y=['fc_by_row_rank_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_by_row_rank_0.tmp_0@GRAD']} = c_allreduce_sum(inputs={X=['fc_by_row_rank_0.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {X@GRAD=['reshape2_3.tmp_0@GRAD'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_row_rank_0.tmp_0@GRAD'], X=['reshape2_3.tmp_0'], Y=['linear_6.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 192], use_quantizer = False)
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_3@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_0'], Out@GRAD=['softmax_0.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['matmul_0.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_0.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_0.tmp_0@GRAD'], X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_2@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_2@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_5.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_1@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_4.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_by_col_rank_0.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 3, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['fc_by_col_rank_0.tmp_0@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_3.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_26.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_26.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['layer_norm_0.b_0@GRAD'], Scale@GRAD=['layer_norm_0.w_0@GRAD'], X@GRAD=['tmp_2@GRAD']} = layer_norm_grad(inputs={Bias=['layer_norm_0.b_0'], Mean=['layer_norm_26.tmp_0'], Scale=['layer_norm_0.w_0'], Variance=['layer_norm_26.tmp_1'], X=['tmp_2'], Y@GRAD=['layer_norm_26.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD', 'layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'], use_mkldnn = False)
    {X@GRAD=['tmp_1@GRAD'], Y@GRAD=['embedding_4.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['tmp_1'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['emb_rank_0.tmp_0@GRAD'], Y@GRAD=['embedding_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['emb_rank_0.tmp_0'], Y=['embedding_3.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {W@GRAD=['embedding_1.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['segment_ids'], Out@GRAD=['embedding_4.tmp_0@GRAD'], W=['embedding_1.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['embedding_0.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['tmp_0'], Out@GRAD=['embedding_3.tmp_0@GRAD'], W=['embedding_0.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['emb_rank_0.tmp_0@GRAD']} = c_allreduce_sum(inputs={X=['emb_rank_0.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 0, use_calc_stream = True)
    {W@GRAD=['embedding_2.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['squeeze_0.tmp_0'], Out@GRAD=['emb_rank_0.tmp_0@GRAD'], W=['embedding_2.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = 7632, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Beta1PowOut=['bert_lm_prediction_head_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['bert_lm_prediction_head_0.b_0_beta2_pow_acc_0'], Moment1Out=['bert_lm_prediction_head_0.b_0_moment1_0'], Moment2Out=['bert_lm_prediction_head_0.b_0_moment2_0'], ParamOut=['bert_lm_prediction_head_0.b_0']} = adam(inputs={Beta1Pow=['bert_lm_prediction_head_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['bert_lm_prediction_head_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['bert_lm_prediction_head_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['bert_lm_prediction_head_0.b_0_moment1_0'], Moment2=['bert_lm_prediction_head_0.b_0_moment2_0'], Param=['bert_lm_prediction_head_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.b_0', 'bert_lm_prediction_head_0.b_0@GRAD'])
    {Out=['tmp_42']} = scale(inputs={ScaleTensor=[], X=['learning_rate_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], scale = 0.009999999776482582)
    {Out=['tmp_43']} = scale(inputs={ScaleTensor=[], X=['tmp_42']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], scale = -1.0)
    {Out=['tmp_44']} = elementwise_mul(inputs={X=['bert_lm_prediction_head_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['bert_lm_prediction_head_0.w_0']} = assign(inputs={X=['tmp_44']}, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'])
    {Beta1PowOut=['bert_lm_prediction_head_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['bert_lm_prediction_head_0.w_0_beta2_pow_acc_0'], Moment1Out=['bert_lm_prediction_head_0.w_0_moment1_0'], Moment2Out=['bert_lm_prediction_head_0.w_0_moment2_0'], ParamOut=['bert_lm_prediction_head_0.w_0']} = adam(inputs={Beta1Pow=['bert_lm_prediction_head_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['bert_lm_prediction_head_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['bert_lm_prediction_head_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['bert_lm_prediction_head_0.w_0_moment1_0'], Moment2=['bert_lm_prediction_head_0.w_0_moment2_0'], Param=['bert_lm_prediction_head_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'])
    {Out=['tmp_45']} = elementwise_mul(inputs={X=['embedding_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_2/weight decay/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_0.w_0']} = assign(inputs={X=['tmp_45']}, op_device = , op_namescope = /optimizer_2/weight decay/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'])
    {Beta1PowOut=['embedding_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_0.w_0_beta2_pow_acc_0'], Moment1Out=['embedding_0.w_0_moment1_0'], Moment2Out=['embedding_0.w_0_moment2_0'], ParamOut=['embedding_0.w_0']} = adam(inputs={Beta1Pow=['embedding_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['embedding_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['embedding_0.w_0_moment1_0'], Moment2=['embedding_0.w_0_moment2_0'], Param=['embedding_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'])
    {Out=['tmp_46']} = elementwise_mul(inputs={X=['embedding_1.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_3/weight decay/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_1.w_0']} = assign(inputs={X=['tmp_46']}, op_device = , op_namescope = /optimizer_3/weight decay/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'])
    {Beta1PowOut=['embedding_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_1.w_0_beta2_pow_acc_0'], Moment1Out=['embedding_1.w_0_moment1_0'], Moment2Out=['embedding_1.w_0_moment2_0'], ParamOut=['embedding_1.w_0']} = adam(inputs={Beta1Pow=['embedding_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['embedding_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['embedding_1.w_0_moment1_0'], Moment2=['embedding_1.w_0_moment2_0'], Param=['embedding_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'])
    {Beta1PowOut=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.b_0_moment1_0'], Moment2Out=['layer_norm_0.b_0_moment2_0'], ParamOut=['layer_norm_0.b_0']} = adam(inputs={Beta1Pow=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['layer_norm_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.b_0_moment1_0'], Moment2=['layer_norm_0.b_0_moment2_0'], Param=['layer_norm_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD'])
    {Beta1PowOut=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.w_0_moment1_0'], Moment2Out=['layer_norm_0.w_0_moment2_0'], ParamOut=['layer_norm_0.w_0']} = adam(inputs={Beta1Pow=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['layer_norm_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.w_0_moment1_0'], Moment2=['layer_norm_0.w_0_moment2_0'], Param=['layer_norm_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'])
    {Beta1PowOut=['linear_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.b_0_beta2_pow_acc_0'], Moment1Out=['linear_0.b_0_moment1_0'], Moment2Out=['linear_0.b_0_moment2_0'], ParamOut=['linear_0.b_0']} = adam(inputs={Beta1Pow=['linear_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_0.b_0_moment1_0'], Moment2=['linear_0.b_0_moment2_0'], Param=['linear_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['linear_0.b_0', 'linear_0.b_0@GRAD'])
    {Out=['tmp_47']} = elementwise_mul(inputs={X=['linear_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_7/weight decay/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_0.w_0']} = assign(inputs={X=['tmp_47']}, op_device = , op_namescope = /optimizer_7/weight decay/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'])
    {Beta1PowOut=['linear_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_beta2_pow_acc_0'], Moment1Out=['linear_0.w_0_moment1_0'], Moment2Out=['linear_0.w_0_moment2_0'], ParamOut=['linear_0.w_0']} = adam(inputs={Beta1Pow=['linear_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_0.w_0_moment1_0'], Moment2=['linear_0.w_0_moment2_0'], Param=['linear_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'])
    {Out=['tmp_48']} = elementwise_mul(inputs={X=['linear_1.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_8/weight decay/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_1.w_0']} = assign(inputs={X=['tmp_48']}, op_device = , op_namescope = /optimizer_8/weight decay/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'])
    {Beta1PowOut=['linear_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_beta2_pow_acc_0'], Moment1Out=['linear_1.w_0_moment1_0'], Moment2Out=['linear_1.w_0_moment2_0'], ParamOut=['linear_1.w_0']} = adam(inputs={Beta1Pow=['linear_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_1.w_0_moment1_0'], Moment2=['linear_1.w_0_moment2_0'], Param=['linear_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'])
    {Out=['tmp_49']} = elementwise_mul(inputs={X=['linear_2.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_9/weight decay/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_2.w_0']} = assign(inputs={X=['tmp_49']}, op_device = , op_namescope = /optimizer_9/weight decay/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'])
    {Beta1PowOut=['linear_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_beta2_pow_acc_0'], Moment1Out=['linear_2.w_0_moment1_0'], Moment2Out=['linear_2.w_0_moment2_0'], ParamOut=['linear_2.w_0']} = adam(inputs={Beta1Pow=['linear_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_2.w_0_moment1_0'], Moment2=['linear_2.w_0_moment2_0'], Param=['linear_2.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'])
}

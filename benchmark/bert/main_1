{ // block 0
    var input_ids : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var segment_ids : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var input_mask : LOD_TENSOR.shape(-1, 1, 1, -1).dtype(FP32).stop_gradient(True)
    var masked_lm_positions : LOD_TENSOR.shape(-1,).dtype(INT64).stop_gradient(True)
    var masked_lm_labels : LOD_TENSOR.shape(-1, 1).dtype(INT64).stop_gradient(True)
    var next_sentence_labels : LOD_TENSOR.shape(-1, 1).dtype(INT64).stop_gradient(True)
    var masked_lm_scale : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(30528, 768).dtype(FP32).stop_gradient(False)
    persist trainable param embedding_1.w_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist trainable param embedding_2.w_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_14.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_15.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_16.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_17.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_18.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_19.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_20.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_21.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_22.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_23.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_24.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_25.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_26.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_27.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_28.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_29.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_30.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_31.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_32.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_33.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_34.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_35.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_36.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_37.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_38.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_39.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_40.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_41.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_42.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_43.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_44.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_45.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_46.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_47.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_48.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_49.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_50.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_51.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_52.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_53.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_54.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_55.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_56.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_57.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_58.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_59.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_60.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_61.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_62.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_63.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_64.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_65.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_66.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_67.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_68.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_69.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_70.w_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist trainable param linear_71.w_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_72.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param linear_72.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_73.w_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_25.w_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param layer_norm_25.b_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist trainable param bert_lm_prediction_head_0.w_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist trainable param bert_lm_prediction_head_0.b_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist trainable param linear_74.w_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var cumsum_0.tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(-1, -1).dtype(INT64).stop_gradient(True)
    var embedding_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_5.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_26.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_26.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_76.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_77.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_0.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_78.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_27.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_27.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_79.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_0.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_80.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_28.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_28.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_81.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_82.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_83.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_1.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_6 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_84.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_29.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_29.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_85.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_1.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_86.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_30.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_30.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_87.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_88.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_89.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_2.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_2.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_90.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_31.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_31.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_91.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_2.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_92.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_32.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_32.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_93.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_94.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_95.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_3.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_3.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_96.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_33.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_33.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_97.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_3.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_98.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_34.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_34.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_99.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_100.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_101.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_4.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_15 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_4.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_102.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_35.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_35.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_103.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_4.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_104.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_36.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_36.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_105.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_106.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_107.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_5.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_5.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_108.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_37.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_37.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_109.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_5.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_110.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_38.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_38.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_111.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_112.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_113.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_6.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_6.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_114.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_39.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_39.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_115.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_6.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_116.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_40.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_40.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_117.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_118.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_119.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_7.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_7.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_120.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_41.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_41.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_121.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_7.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_122.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_42.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_42.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_123.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_124.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_125.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_8.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_8.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_126.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_28 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_43.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_43.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_127.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_8.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_128.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_44.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_44.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_129.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_130.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_131.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_9.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_9.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_9.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_132.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_45.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_45.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_133.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_9.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_134.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_46.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_46.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_135.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_136.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_137.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_10.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_10.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_10.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_138.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_47.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_47.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_139.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_10.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_140.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_48.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_48.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_141.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_142.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_143.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_11.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_11.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_11.tmp_0 : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_0 : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_1 : LOD_TENSOR.shape(0, -1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_144.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_37 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_49.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_49.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_145.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_11.tmp_0 : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_146.tmp_0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_50.tmp_1 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var layer_norm_50.tmp_2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2_slice_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_147.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_147.tmp_1 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var tanh_1.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_1 : LOD_TENSOR.shape(0, -1, -1, 768).dtype(FP32).stop_gradient(False)
    var gather_0.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_148.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var gelu_12.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_12.tmp_0 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var linear_149.tmp_0 : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_0 : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(30528, 768).dtype(FP32).stop_gradient(False)
    var embedding_1.w_0@GRAD : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    var embedding_2.w_0@GRAD : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    var embedding_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var embedding_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_1@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_0.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_75.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_77.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_78.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_83.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_84.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_86.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_14.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_88.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_15.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_16.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var linear_79.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_91.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_17.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var linear_92.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_18.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_93.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_20.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_94.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_95.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var matmul_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_96.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_97.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_23.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var gelu_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_98.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_24.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_25.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_26.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_100.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_101.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_27.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_28.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var transpose_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_110.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_99.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_105.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var sum_0.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var linear_64.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_45.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var softmax_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var bert_lm_prediction_head_0.b_0@GRAD : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    var gelu_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_40.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var reshape2_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_120.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var gelu_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_102.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_80.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var gelu_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_72.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_76.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_81.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_113.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_37.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_138.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_131.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_53.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_22.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var linear_42.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_122.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_36.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var matmul_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_33.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_66.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_129.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_43.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var gather_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_123.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_73.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var transpose_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_89.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_147.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_34.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var bert_lm_prediction_head_0.w_0@GRAD : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    var reshape2_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_87.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_69.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var reshape2_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_126.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_117.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_0.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_82.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_118.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_56.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_90.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_124.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2_slice_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var transpose_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_62.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_1.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var linear_149.tmp_0@GRAD : LOD_TENSOR.shape(-1, 2).dtype(FP32).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var tanh_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_61.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_74.w_0@GRAD : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    var matmul_v2_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 30528).dtype(FP32).stop_gradient(False)
    var transpose_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var gelu_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var matmul_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_148.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_48.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_130.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_50.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_147.tmp_1@GRAD : LOD_TENSOR.shape(-1, 768).dtype(FP32).stop_gradient(False)
    var linear_72.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_146.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_145.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_70.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var linear_144.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_57.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var gelu_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var embedding_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_143.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_142.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_68.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_67.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_49.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_140.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_65.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_63.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_38.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var transpose_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var softmax_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var linear_137.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_136.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_135.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_21.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_31.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_60.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_49.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_115.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_32.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_134.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_59.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var reshape2_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_133.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_58.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_132.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_141.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var softmax_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var matmul_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var reshape2_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_55.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_116.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_54.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var gelu_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_127.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_52.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var linear_51.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var transpose_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var reshape2_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_85.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var gelu_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var softmax_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var reshape2_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var softmax_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var linear_125.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_50.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_139.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_48.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_71.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var matmul_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_47.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_121.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var softmax_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_46.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(FP32).stop_gradient(False)
    var linear_119.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_44.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var transpose_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_114.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_39.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var matmul_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, -1).dtype(FP32).stop_gradient(False)
    var transpose_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_112.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_111.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_41.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var gelu_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var linear_109.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_35.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var linear_30.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_108.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var reshape2_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var transpose_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 12, 64).dtype(FP32).stop_gradient(False)
    var linear_19.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var transpose_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, -1, 64).dtype(FP32).stop_gradient(False)
    var linear_128.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_107.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_106.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_104.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    var linear_103.tmp_0@GRAD : LOD_TENSOR.shape(-1, -1, 3072).dtype(FP32).stop_gradient(False)
    var linear_29.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, -1, 768).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_moment1_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_moment2_0 : LOD_TENSOR.shape(30528,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var bert_lm_prediction_head_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_moment1_0 : LOD_TENSOR.shape(30528, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_moment2_0 : LOD_TENSOR.shape(30528, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_moment1_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_moment2_0 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_2.w_0_moment1_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_2.w_0_moment2_0 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    persist var embedding_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var embedding_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_10.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_10.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_10.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_10.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_11.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_11.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_11.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_11.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_12.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_12.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_12.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_12.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_13.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_13.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_13.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_13.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_14.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_14.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_14.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_14.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_15.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_15.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_15.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_15.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_16.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_16.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_16.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_16.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_17.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_17.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_17.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_17.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_18.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_18.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_18.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_18.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_19.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_19.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_19.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_19.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_20.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_20.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_20.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_20.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_21.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_21.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_21.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_21.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_22.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_22.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_22.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_22.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_23.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_23.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_23.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_23.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_24.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_24.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_24.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_24.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_25.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_25.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_25.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_25.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_26.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_26.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_26.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_26.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_27.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_27.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_27.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_27.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_28.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_28.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_28.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_28.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_29.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_29.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_29.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_29.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_3.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_3.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_3.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_3.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_30.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_30.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_30.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_30.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_31.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_31.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_31.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_31.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_32.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_32.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_32.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_32.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_33.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_33.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_33.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_33.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_34.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_34.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_34.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_34.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_35.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_35.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_35.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_35.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_36.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_36.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_36.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_36.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_37.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_37.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_37.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_37.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_38.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_38.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_38.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_38.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_39.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_39.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_39.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_39.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_4.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_4.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_4.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_4.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_40.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_40.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_40.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_40.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_41.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_41.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_41.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_41.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_42.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_42.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_42.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_42.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_43.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_43.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_43.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_43.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_44.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_44.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_44.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_44.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_45.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_45.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_45.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_45.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_46.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_46.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_46.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_46.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_47.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_47.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_47.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_47.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_48.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_48.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_48.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_48.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_49.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_49.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_49.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_49.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_5.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_5.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_5.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_5.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_50.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_50.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_50.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_50.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_51.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_51.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_51.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_51.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_52.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_52.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_52.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_52.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_53.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_53.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_53.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_53.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_54.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_54.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_54.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_54.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_55.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_55.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_55.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_55.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_56.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_56.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_56.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_56.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_57.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_57.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_57.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_57.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_58.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_58.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_58.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_58.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_59.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_59.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_59.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_59.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_6.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_6.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_6.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_6.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_60.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_60.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_60.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_60.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_61.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_61.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_61.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_61.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_62.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_62.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_62.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_62.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_63.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_63.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_63.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_63.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_64.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_64.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_64.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_64.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_65.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_65.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_65.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_65.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_66.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_66.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_66.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_66.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_67.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_67.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_67.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_67.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_68.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_68.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_68.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_68.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_69.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_69.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_69.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_69.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_7.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_7.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_7.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_7.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_70.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_70.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    persist var linear_70.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_70.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_71.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_71.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    persist var linear_71.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_71.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_72.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var linear_72.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(FP32).stop_gradient(False)
    persist var linear_72.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_72.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_72.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_72.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_72.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_72.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_73.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_73.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_73.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_73.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_74.w_0_moment1_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    persist var linear_74.w_0_moment2_0 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    persist var linear_74.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_74.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_8.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_8.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_8.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_8.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_9.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_9.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    persist var linear_9.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var linear_9.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(True)
    var tmp_42 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(1,).dtype(FP32).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(768, 30528).dtype(FP32).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(30528, 768).dtype(FP32).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(512, 768).dtype(FP32).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(2, 768).dtype(FP32).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_50 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_51 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_52 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_53 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_54 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_55 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_56 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_57 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_58 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_59 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_60 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_61 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_62 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_63 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_64 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_65 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_66 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_67 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_68 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_69 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_70 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_71 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_72 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_73 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_74 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_75 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_76 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_77 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_78 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_79 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_80 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_81 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_82 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_83 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_84 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_85 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_86 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_87 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_88 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_89 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_90 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_91 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_92 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_93 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_94 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_95 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_96 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_97 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_98 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_99 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_100 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_101 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_102 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_103 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_104 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_105 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_106 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_107 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_108 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_109 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_110 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_111 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_112 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_113 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_114 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_115 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_116 : LOD_TENSOR.shape(768, 3072).dtype(FP32).stop_gradient(False)
    var tmp_117 : LOD_TENSOR.shape(3072, 768).dtype(FP32).stop_gradient(False)
    var tmp_118 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_119 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_120 : LOD_TENSOR.shape(768, 2).dtype(FP32).stop_gradient(False)
    var tmp_121 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)
    var tmp_122 : LOD_TENSOR.shape(768, 768).dtype(FP32).stop_gradient(False)

    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['input_ids']}, dtype = 3, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 1.0)
    {Out=['cumsum_0.tmp_0']} = cumsum(inputs={X=['full_like_0.tmp_0']}, axis = 1, exclusive = False, flatten = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], reverse = False)
    {Out=['tmp_0']} = elementwise_sub(inputs={X=['cumsum_0.tmp_0'], Y=['full_like_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_3.tmp_0']} = lookup_table_v2(inputs={Ids=['input_ids'], W=['embedding_0.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_4.tmp_0']} = lookup_table_v2(inputs={Ids=['tmp_0'], W=['embedding_1.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_5.tmp_0']} = lookup_table_v2(inputs={Ids=['segment_ids'], W=['embedding_2.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['tmp_1']} = elementwise_add(inputs={X=['embedding_3.tmp_0'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_2']} = elementwise_add(inputs={X=['tmp_1'], Y=['embedding_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_26.tmp_0'], Variance=['layer_norm_26.tmp_1'], Y=['layer_norm_26.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_0.b_0'], Scale=['layer_norm_0.w_0'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_75.tmp_0']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_0.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_75.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['reshape2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_76.tmp_0']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_1.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_77.tmp_0']} = matmul(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_2.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_76.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['reshape2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_77.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_0.tmp_0']} = matmul(inputs={X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_3']} = elementwise_add(inputs={X=['matmul_0.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0']} = softmax(inputs={X=['tmp_3']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_v2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_78.tmp_0']} = matmul(inputs={X=['reshape2_3.tmp_0'], Y=['linear_3.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_4']} = elementwise_add(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_78.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_27.tmp_0'], Variance=['layer_norm_27.tmp_1'], Y=['layer_norm_27.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_79.tmp_0']} = matmul(inputs={X=['layer_norm_27.tmp_2'], Y=['linear_4.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_0.tmp_0']} = gelu(inputs={X=['linear_79.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_80.tmp_0']} = matmul(inputs={X=['gelu_0.tmp_0'], Y=['linear_5.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_5']} = elementwise_add(inputs={X=['layer_norm_27.tmp_2'], Y=['linear_80.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_28.tmp_0'], Variance=['layer_norm_28.tmp_1'], Y=['layer_norm_28.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_81.tmp_0']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_6.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_81.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['reshape2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_82.tmp_0']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_7.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_83.tmp_0']} = matmul(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_8.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_82.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['reshape2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_83.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['reshape2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_1.tmp_0']} = matmul(inputs={X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_6']} = elementwise_add(inputs={X=['matmul_1.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_1.tmp_0']} = softmax(inputs={X=['tmp_6']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_84.tmp_0']} = matmul(inputs={X=['reshape2_7.tmp_0'], Y=['linear_9.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_7']} = elementwise_add(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_84.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_29.tmp_0'], Variance=['layer_norm_29.tmp_1'], Y=['layer_norm_29.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_7']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_85.tmp_0']} = matmul(inputs={X=['layer_norm_29.tmp_2'], Y=['linear_10.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_1.tmp_0']} = gelu(inputs={X=['linear_85.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_86.tmp_0']} = matmul(inputs={X=['gelu_1.tmp_0'], Y=['linear_11.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_8']} = elementwise_add(inputs={X=['layer_norm_29.tmp_2'], Y=['linear_86.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_30.tmp_0'], Variance=['layer_norm_30.tmp_1'], Y=['layer_norm_30.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_87.tmp_0']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_12.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_87.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['reshape2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_88.tmp_0']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_13.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_89.tmp_0']} = matmul(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_14.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_88.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['reshape2_9.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_89.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['reshape2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_2.tmp_0']} = matmul(inputs={X=['transpose_8.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_9']} = elementwise_add(inputs={X=['matmul_2.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_2.tmp_0']} = softmax(inputs={X=['tmp_9']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['matmul_v2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_90.tmp_0']} = matmul(inputs={X=['reshape2_11.tmp_0'], Y=['linear_15.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_10']} = elementwise_add(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_90.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_31.tmp_0'], Variance=['layer_norm_31.tmp_1'], Y=['layer_norm_31.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_91.tmp_0']} = matmul(inputs={X=['layer_norm_31.tmp_2'], Y=['linear_16.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_2.tmp_0']} = gelu(inputs={X=['linear_91.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_92.tmp_0']} = matmul(inputs={X=['gelu_2.tmp_0'], Y=['linear_17.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_11']} = elementwise_add(inputs={X=['layer_norm_31.tmp_2'], Y=['linear_92.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_32.tmp_0'], Variance=['layer_norm_32.tmp_1'], Y=['layer_norm_32.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_11']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_93.tmp_0']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_18.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_93.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['reshape2_12.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_94.tmp_0']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_19.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_95.tmp_0']} = matmul(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_20.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_13.tmp_0'], XShape=['reshape2_13.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_94.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['reshape2_13.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_14.tmp_0'], XShape=['reshape2_14.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_95.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['reshape2_14.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_3.tmp_0']} = matmul(inputs={X=['transpose_12.tmp_0'], Y=['transpose_13.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_12']} = elementwise_add(inputs={X=['matmul_3.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_3.tmp_0']} = softmax(inputs={X=['tmp_12']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_15.tmp_0'], XShape=['reshape2_15.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_96.tmp_0']} = matmul(inputs={X=['reshape2_15.tmp_0'], Y=['linear_21.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_13']} = elementwise_add(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_96.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_33.tmp_0'], Variance=['layer_norm_33.tmp_1'], Y=['layer_norm_33.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_13']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_97.tmp_0']} = matmul(inputs={X=['layer_norm_33.tmp_2'], Y=['linear_22.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_3.tmp_0']} = gelu(inputs={X=['linear_97.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_98.tmp_0']} = matmul(inputs={X=['gelu_3.tmp_0'], Y=['linear_23.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_14']} = elementwise_add(inputs={X=['layer_norm_33.tmp_2'], Y=['linear_98.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_34.tmp_0'], Variance=['layer_norm_34.tmp_1'], Y=['layer_norm_34.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_14']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_99.tmp_0']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_24.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_16.tmp_0'], XShape=['reshape2_16.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_99.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['reshape2_16.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_100.tmp_0']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_25.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_101.tmp_0']} = matmul(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_26.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_17.tmp_0'], XShape=['reshape2_17.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_100.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['reshape2_17.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_18.tmp_0'], XShape=['reshape2_18.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_101.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['reshape2_18.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_4.tmp_0']} = matmul(inputs={X=['transpose_16.tmp_0'], Y=['transpose_17.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_15']} = elementwise_add(inputs={X=['matmul_4.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_4.tmp_0']} = softmax(inputs={X=['tmp_15']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_19.tmp_0'], XShape=['reshape2_19.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_102.tmp_0']} = matmul(inputs={X=['reshape2_19.tmp_0'], Y=['linear_27.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_16']} = elementwise_add(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_102.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_35.tmp_0'], Variance=['layer_norm_35.tmp_1'], Y=['layer_norm_35.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_16']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_103.tmp_0']} = matmul(inputs={X=['layer_norm_35.tmp_2'], Y=['linear_28.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_4.tmp_0']} = gelu(inputs={X=['linear_103.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_104.tmp_0']} = matmul(inputs={X=['gelu_4.tmp_0'], Y=['linear_29.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_17']} = elementwise_add(inputs={X=['layer_norm_35.tmp_2'], Y=['linear_104.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_36.tmp_0'], Variance=['layer_norm_36.tmp_1'], Y=['layer_norm_36.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_17']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_105.tmp_0']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_30.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_20.tmp_0'], XShape=['reshape2_20.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_105.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_20.tmp_0'], XShape=['transpose_20.tmp_1']} = transpose2(inputs={X=['reshape2_20.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_106.tmp_0']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_31.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_107.tmp_0']} = matmul(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_32.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_21.tmp_0'], XShape=['reshape2_21.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_106.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_21.tmp_0'], XShape=['transpose_21.tmp_1']} = transpose2(inputs={X=['reshape2_21.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_22.tmp_0'], XShape=['reshape2_22.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_107.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_22.tmp_0'], XShape=['transpose_22.tmp_1']} = transpose2(inputs={X=['reshape2_22.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_5.tmp_0']} = matmul(inputs={X=['transpose_20.tmp_0'], Y=['transpose_21.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_18']} = elementwise_add(inputs={X=['matmul_5.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_5.tmp_0']} = softmax(inputs={X=['tmp_18']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_23.tmp_0'], XShape=['transpose_23.tmp_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_23.tmp_0'], XShape=['reshape2_23.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_108.tmp_0']} = matmul(inputs={X=['reshape2_23.tmp_0'], Y=['linear_33.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_19']} = elementwise_add(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_108.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_37.tmp_0'], Variance=['layer_norm_37.tmp_1'], Y=['layer_norm_37.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_19']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_109.tmp_0']} = matmul(inputs={X=['layer_norm_37.tmp_2'], Y=['linear_34.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_5.tmp_0']} = gelu(inputs={X=['linear_109.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_110.tmp_0']} = matmul(inputs={X=['gelu_5.tmp_0'], Y=['linear_35.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_20']} = elementwise_add(inputs={X=['layer_norm_37.tmp_2'], Y=['linear_110.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_38.tmp_0'], Variance=['layer_norm_38.tmp_1'], Y=['layer_norm_38.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_20']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_111.tmp_0']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_36.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_24.tmp_0'], XShape=['reshape2_24.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_111.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_24.tmp_0'], XShape=['transpose_24.tmp_1']} = transpose2(inputs={X=['reshape2_24.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_112.tmp_0']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_37.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_113.tmp_0']} = matmul(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_38.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_25.tmp_0'], XShape=['reshape2_25.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_112.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_25.tmp_0'], XShape=['transpose_25.tmp_1']} = transpose2(inputs={X=['reshape2_25.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_26.tmp_0'], XShape=['reshape2_26.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_113.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_26.tmp_0'], XShape=['transpose_26.tmp_1']} = transpose2(inputs={X=['reshape2_26.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_6.tmp_0']} = matmul(inputs={X=['transpose_24.tmp_0'], Y=['transpose_25.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_21']} = elementwise_add(inputs={X=['matmul_6.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_6.tmp_0']} = softmax(inputs={X=['tmp_21']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_27.tmp_0'], XShape=['transpose_27.tmp_1']} = transpose2(inputs={X=['matmul_v2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_27.tmp_0'], XShape=['reshape2_27.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_27.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_114.tmp_0']} = matmul(inputs={X=['reshape2_27.tmp_0'], Y=['linear_39.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_22']} = elementwise_add(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_114.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_39.tmp_0'], Variance=['layer_norm_39.tmp_1'], Y=['layer_norm_39.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_22']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_115.tmp_0']} = matmul(inputs={X=['layer_norm_39.tmp_2'], Y=['linear_40.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_6.tmp_0']} = gelu(inputs={X=['linear_115.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_116.tmp_0']} = matmul(inputs={X=['gelu_6.tmp_0'], Y=['linear_41.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_23']} = elementwise_add(inputs={X=['layer_norm_39.tmp_2'], Y=['linear_116.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_40.tmp_0'], Variance=['layer_norm_40.tmp_1'], Y=['layer_norm_40.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_23']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_117.tmp_0']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_42.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_28.tmp_0'], XShape=['reshape2_28.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_117.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_28.tmp_0'], XShape=['transpose_28.tmp_1']} = transpose2(inputs={X=['reshape2_28.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_118.tmp_0']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_43.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_119.tmp_0']} = matmul(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_44.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_29.tmp_0'], XShape=['reshape2_29.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_118.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_29.tmp_0'], XShape=['transpose_29.tmp_1']} = transpose2(inputs={X=['reshape2_29.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_30.tmp_0'], XShape=['reshape2_30.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_119.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_30.tmp_0'], XShape=['transpose_30.tmp_1']} = transpose2(inputs={X=['reshape2_30.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_7.tmp_0']} = matmul(inputs={X=['transpose_28.tmp_0'], Y=['transpose_29.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_24']} = elementwise_add(inputs={X=['matmul_7.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_7.tmp_0']} = softmax(inputs={X=['tmp_24']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_31.tmp_0'], XShape=['transpose_31.tmp_1']} = transpose2(inputs={X=['matmul_v2_7.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_31.tmp_0'], XShape=['reshape2_31.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_31.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_120.tmp_0']} = matmul(inputs={X=['reshape2_31.tmp_0'], Y=['linear_45.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_25']} = elementwise_add(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_120.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_41.tmp_0'], Variance=['layer_norm_41.tmp_1'], Y=['layer_norm_41.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_25']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_121.tmp_0']} = matmul(inputs={X=['layer_norm_41.tmp_2'], Y=['linear_46.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_7.tmp_0']} = gelu(inputs={X=['linear_121.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_122.tmp_0']} = matmul(inputs={X=['gelu_7.tmp_0'], Y=['linear_47.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_26']} = elementwise_add(inputs={X=['layer_norm_41.tmp_2'], Y=['linear_122.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_42.tmp_0'], Variance=['layer_norm_42.tmp_1'], Y=['layer_norm_42.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_26']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_123.tmp_0']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_48.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_32.tmp_0'], XShape=['reshape2_32.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_123.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_32.tmp_0'], XShape=['transpose_32.tmp_1']} = transpose2(inputs={X=['reshape2_32.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_124.tmp_0']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_49.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_125.tmp_0']} = matmul(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_50.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_33.tmp_0'], XShape=['reshape2_33.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_124.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_33.tmp_0'], XShape=['transpose_33.tmp_1']} = transpose2(inputs={X=['reshape2_33.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_34.tmp_0'], XShape=['reshape2_34.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_125.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_34.tmp_0'], XShape=['transpose_34.tmp_1']} = transpose2(inputs={X=['reshape2_34.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_8.tmp_0']} = matmul(inputs={X=['transpose_32.tmp_0'], Y=['transpose_33.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_27']} = elementwise_add(inputs={X=['matmul_8.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_8.tmp_0']} = softmax(inputs={X=['tmp_27']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_35.tmp_0'], XShape=['transpose_35.tmp_1']} = transpose2(inputs={X=['matmul_v2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_35.tmp_0'], XShape=['reshape2_35.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_35.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_126.tmp_0']} = matmul(inputs={X=['reshape2_35.tmp_0'], Y=['linear_51.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_28']} = elementwise_add(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_126.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_43.tmp_0'], Variance=['layer_norm_43.tmp_1'], Y=['layer_norm_43.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_28']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_127.tmp_0']} = matmul(inputs={X=['layer_norm_43.tmp_2'], Y=['linear_52.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_8.tmp_0']} = gelu(inputs={X=['linear_127.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_128.tmp_0']} = matmul(inputs={X=['gelu_8.tmp_0'], Y=['linear_53.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_29']} = elementwise_add(inputs={X=['layer_norm_43.tmp_2'], Y=['linear_128.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_44.tmp_0'], Variance=['layer_norm_44.tmp_1'], Y=['layer_norm_44.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_29']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_129.tmp_0']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_54.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_36.tmp_0'], XShape=['reshape2_36.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_129.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_36.tmp_0'], XShape=['transpose_36.tmp_1']} = transpose2(inputs={X=['reshape2_36.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_130.tmp_0']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_55.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_131.tmp_0']} = matmul(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_56.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_37.tmp_0'], XShape=['reshape2_37.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_130.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_37.tmp_0'], XShape=['transpose_37.tmp_1']} = transpose2(inputs={X=['reshape2_37.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_38.tmp_0'], XShape=['reshape2_38.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_131.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_38.tmp_0'], XShape=['transpose_38.tmp_1']} = transpose2(inputs={X=['reshape2_38.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_9.tmp_0']} = matmul(inputs={X=['transpose_36.tmp_0'], Y=['transpose_37.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_30']} = elementwise_add(inputs={X=['matmul_9.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_9.tmp_0']} = softmax(inputs={X=['tmp_30']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_9.tmp_0']} = matmul_v2(inputs={X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_39.tmp_0'], XShape=['transpose_39.tmp_1']} = transpose2(inputs={X=['matmul_v2_9.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_39.tmp_0'], XShape=['reshape2_39.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_39.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_132.tmp_0']} = matmul(inputs={X=['reshape2_39.tmp_0'], Y=['linear_57.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_31']} = elementwise_add(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_132.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_45.tmp_0'], Variance=['layer_norm_45.tmp_1'], Y=['layer_norm_45.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_31']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_133.tmp_0']} = matmul(inputs={X=['layer_norm_45.tmp_2'], Y=['linear_58.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_9.tmp_0']} = gelu(inputs={X=['linear_133.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_134.tmp_0']} = matmul(inputs={X=['gelu_9.tmp_0'], Y=['linear_59.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_32']} = elementwise_add(inputs={X=['layer_norm_45.tmp_2'], Y=['linear_134.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_46.tmp_0'], Variance=['layer_norm_46.tmp_1'], Y=['layer_norm_46.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_32']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_135.tmp_0']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_60.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_40.tmp_0'], XShape=['reshape2_40.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_135.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_40.tmp_0'], XShape=['transpose_40.tmp_1']} = transpose2(inputs={X=['reshape2_40.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_136.tmp_0']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_61.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_137.tmp_0']} = matmul(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_62.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_41.tmp_0'], XShape=['reshape2_41.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_136.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_41.tmp_0'], XShape=['transpose_41.tmp_1']} = transpose2(inputs={X=['reshape2_41.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_42.tmp_0'], XShape=['reshape2_42.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_137.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_42.tmp_0'], XShape=['transpose_42.tmp_1']} = transpose2(inputs={X=['reshape2_42.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_10.tmp_0']} = matmul(inputs={X=['transpose_40.tmp_0'], Y=['transpose_41.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_33']} = elementwise_add(inputs={X=['matmul_10.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_10.tmp_0']} = softmax(inputs={X=['tmp_33']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_10.tmp_0']} = matmul_v2(inputs={X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_43.tmp_0'], XShape=['transpose_43.tmp_1']} = transpose2(inputs={X=['matmul_v2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_43.tmp_0'], XShape=['reshape2_43.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_43.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_138.tmp_0']} = matmul(inputs={X=['reshape2_43.tmp_0'], Y=['linear_63.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_34']} = elementwise_add(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_138.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_47.tmp_0'], Variance=['layer_norm_47.tmp_1'], Y=['layer_norm_47.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_34']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_139.tmp_0']} = matmul(inputs={X=['layer_norm_47.tmp_2'], Y=['linear_64.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_10.tmp_0']} = gelu(inputs={X=['linear_139.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_140.tmp_0']} = matmul(inputs={X=['gelu_10.tmp_0'], Y=['linear_65.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_35']} = elementwise_add(inputs={X=['layer_norm_47.tmp_2'], Y=['linear_140.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_48.tmp_0'], Variance=['layer_norm_48.tmp_1'], Y=['layer_norm_48.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_35']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_141.tmp_0']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_66.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_44.tmp_0'], XShape=['reshape2_44.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_141.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_44.tmp_0'], XShape=['transpose_44.tmp_1']} = transpose2(inputs={X=['reshape2_44.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['linear_142.tmp_0']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_67.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_143.tmp_0']} = matmul(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_68.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_45.tmp_0'], XShape=['reshape2_45.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_142.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_45.tmp_0'], XShape=['transpose_45.tmp_1']} = transpose2(inputs={X=['reshape2_45.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_46.tmp_0'], XShape=['reshape2_46.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_143.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_46.tmp_0'], XShape=['transpose_46.tmp_1']} = transpose2(inputs={X=['reshape2_46.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['matmul_11.tmp_0']} = matmul(inputs={X=['transpose_44.tmp_0'], Y=['transpose_45.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_36']} = elementwise_add(inputs={X=['matmul_11.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_11.tmp_0']} = softmax(inputs={X=['tmp_36']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_11.tmp_0']} = matmul_v2(inputs={X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['transpose_47.tmp_0'], XShape=['transpose_47.tmp_1']} = transpose2(inputs={X=['matmul_v2_11.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_47.tmp_0'], XShape=['reshape2_47.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_47.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['linear_144.tmp_0']} = matmul(inputs={X=['reshape2_47.tmp_0'], Y=['linear_69.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_37']} = elementwise_add(inputs={X=['layer_norm_48.tmp_2'], Y=['linear_144.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_49.tmp_0'], Variance=['layer_norm_49.tmp_1'], Y=['layer_norm_49.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_37']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['linear_145.tmp_0']} = matmul(inputs={X=['layer_norm_49.tmp_2'], Y=['linear_70.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_11.tmp_0']} = gelu(inputs={X=['linear_145.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['linear_146.tmp_0']} = matmul(inputs={X=['gelu_11.tmp_0'], Y=['linear_71.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_38']} = elementwise_add(inputs={X=['layer_norm_49.tmp_2'], Y=['linear_146.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_50.tmp_0'], Variance=['layer_norm_50.tmp_1'], Y=['layer_norm_50.tmp_2']} = layer_norm(inputs={Bias=[], Scale=[], X=['tmp_38']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['layer_norm_50.tmp_2_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['layer_norm_50.tmp_2'], StartsTensor=[], StartsTensorList=[]}, axes = [1], decrease_axis = [1], ends = [1], infer_flags = [1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], starts = [0])
    {Out=['linear_147.tmp_0']} = matmul(inputs={X=['layer_norm_50.tmp_2_slice_0'], Y=['linear_72.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['linear_147.tmp_1']} = elementwise_add(inputs={X=['linear_147.tmp_0'], Y=['linear_72.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tanh_1.tmp_0']} = tanh(inputs={X=['linear_147.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['reshape2_48.tmp_0'], XShape=['reshape2_48.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['layer_norm_50.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Out=['gather_0.tmp_0']} = gather(inputs={Axis=[], Index=['masked_lm_positions'], X=['reshape2_48.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], overwrite = False)
    {Out=['linear_148.tmp_0']} = matmul(inputs={X=['gather_0.tmp_0'], Y=['linear_73.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['gelu_12.tmp_0']} = gelu(inputs={X=['linear_148.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['matmul_v2_12.tmp_0']} = matmul_v2(inputs={X=['gelu_12.tmp_0'], Y=['bert_lm_prediction_head_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False)
    {Out=['tmp_39']} = elementwise_add(inputs={X=['matmul_v2_12.tmp_0'], Y=['bert_lm_prediction_head_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_149.tmp_0']} = matmul(inputs={X=['tanh_1.tmp_0'], Y=['linear_74.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['masked_lm_labels'], Logits=['tmp_39']}, axis = -1, ignore_index = -1, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False)
    {Out=['tmp_40']} = elementwise_div(inputs={X=['softmax_with_cross_entropy_0.tmp_1'], Y=['masked_lm_scale']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Loss=['softmax_with_cross_entropy_1.tmp_1'], Softmax=['softmax_with_cross_entropy_1.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['next_sentence_labels'], Logits=['linear_149.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False)
    {Out=['sum_0.tmp_0']} = reduce_sum(inputs={X=['tmp_40']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['softmax_with_cross_entropy_1.tmp_1']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Out=['tmp_41']} = elementwise_add(inputs={X=['sum_0.tmp_0'], Y=['mean_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 256, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_41@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_role = 257, shape = [1], value = 1.0)
    {X@GRAD=['sum_0.tmp_0@GRAD'], Y@GRAD=['mean_0.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_41@GRAD'], X=['sum_0.tmp_0'], Y=['mean_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['softmax_with_cross_entropy_1.tmp_1@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['softmax_with_cross_entropy_1.tmp_1']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True)
    {X@GRAD=['tmp_40@GRAD']} = reduce_sum_grad(inputs={Out@GRAD=['sum_0.tmp_0@GRAD'], X=['tmp_40']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True)
    {Logits@GRAD=['linear_149.tmp_0@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['next_sentence_labels'], Loss@GRAD=['softmax_with_cross_entropy_1.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_1.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False)
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Y@GRAD=[]} = elementwise_div_grad(inputs={Out=['tmp_40'], Out@GRAD=['tmp_40@GRAD'], X=['softmax_with_cross_entropy_0.tmp_1'], Y=['masked_lm_scale']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Logits@GRAD=['tmp_39@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['masked_lm_labels'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -1, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False)
    {X@GRAD=['tanh_1.tmp_0@GRAD'], Y@GRAD=['linear_74.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_149.tmp_0@GRAD'], X=['tanh_1.tmp_0'], Y=['linear_74.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_74.w_0', 'linear_74.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['matmul_v2_12.tmp_0@GRAD'], Y@GRAD=['bert_lm_prediction_head_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['matmul_v2_12.tmp_0'], Y=['bert_lm_prediction_head_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['bert_lm_prediction_head_0.b_0', 'bert_lm_prediction_head_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_12.tmp_0@GRAD'], Y@GRAD=['bert_lm_prediction_head_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_12.tmp_0@GRAD'], X=['gelu_12.tmp_0'], Y=['bert_lm_prediction_head_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], trans_x = False, trans_y = False)
    {X@GRAD=['linear_148.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_12.tmp_0@GRAD'], X=['linear_148.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['gather_0.tmp_0@GRAD'], Y@GRAD=['linear_73.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_148.tmp_0@GRAD'], X=['gather_0.tmp_0'], Y=['linear_73.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_73.w_0', 'linear_73.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_48.tmp_0@GRAD']} = gather_grad(inputs={Axis=[], Index=['masked_lm_positions'], Out@GRAD=['gather_0.tmp_0@GRAD'], X=['reshape2_48.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], overwrite = False)
    {X@GRAD=['layer_norm_50.tmp_2@GRAD@RENAME@block0@0']} = reshape2_grad(inputs={Out@GRAD=['reshape2_48.tmp_0@GRAD'], XShape=['reshape2_48.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {X@GRAD=['linear_147.tmp_1@GRAD']} = tanh_grad(inputs={Out=['tanh_1.tmp_0'], Out@GRAD=['tanh_1.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['linear_147.tmp_0@GRAD'], Y@GRAD=['linear_72.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_147.tmp_1@GRAD'], X=['linear_147.tmp_0'], Y=['linear_72.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_72.b_0', 'linear_72.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_50.tmp_2_slice_0@GRAD'], Y@GRAD=['linear_72.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_147.tmp_0@GRAD'], X=['layer_norm_50.tmp_2_slice_0'], Y=['linear_72.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_72.w_0', 'linear_72.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Input@GRAD=['layer_norm_50.tmp_2@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['layer_norm_50.tmp_2'], Out@GRAD=['layer_norm_50.tmp_2_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [1], decrease_axis = [1], ends = [1], infer_flags = [1], op_device = , op_namescope = /, op_role = 1, op_role_var = [], starts = [0])
    {Out=['layer_norm_50.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_50.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_50.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_38@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_50.tmp_0'], Scale=[], Variance=['layer_norm_50.tmp_1'], X=['tmp_38'], Y@GRAD=['layer_norm_50.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_49.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_146.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_38@GRAD'], X=['layer_norm_49.tmp_2'], Y=['linear_146.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_11.tmp_0@GRAD'], Y@GRAD=['linear_71.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_146.tmp_0@GRAD'], X=['gelu_11.tmp_0'], Y=['linear_71.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_71.w_0', 'linear_71.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_145.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_11.tmp_0@GRAD'], X=['linear_145.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_49.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_70.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_145.tmp_0@GRAD'], X=['layer_norm_49.tmp_2'], Y=['linear_70.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_70.w_0', 'linear_70.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_49.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_49.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_49.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_37@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_49.tmp_0'], Scale=[], Variance=['layer_norm_49.tmp_1'], X=['tmp_37'], Y@GRAD=['layer_norm_49.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_144.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_144.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_47.tmp_0@GRAD'], Y@GRAD=['linear_69.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_144.tmp_0@GRAD'], X=['reshape2_47.tmp_0'], Y=['linear_69.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_69.w_0', 'linear_69.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_47.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_47.tmp_0@GRAD'], XShape=['reshape2_47.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_47.tmp_0@GRAD'], XShape=['transpose_47.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_11.tmp_0@GRAD'], Y@GRAD=['transpose_46.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_11.tmp_0@GRAD'], X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_36@GRAD']} = softmax_grad(inputs={Out=['softmax_11.tmp_0'], Out@GRAD=['softmax_11.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_11.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_36@GRAD'], X=['matmul_11.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_44.tmp_0@GRAD'], Y@GRAD=['transpose_45.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_11.tmp_0@GRAD'], X=['transpose_44.tmp_0'], Y=['transpose_45.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_46.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_46.tmp_0@GRAD'], XShape=['transpose_46.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_143.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_46.tmp_0@GRAD'], XShape=['reshape2_46.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_45.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_45.tmp_0@GRAD'], XShape=['transpose_45.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_142.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_45.tmp_0@GRAD'], XShape=['reshape2_45.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_68.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_143.tmp_0@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_68.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_68.w_0', 'linear_68.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_67.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_142.tmp_0@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_67.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_67.w_0', 'linear_67.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_44.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_44.tmp_0@GRAD'], XShape=['transpose_44.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_141.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_44.tmp_0@GRAD'], XShape=['reshape2_44.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_66.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_141.tmp_0@GRAD'], X=['layer_norm_48.tmp_2'], Y=['linear_66.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_66.w_0', 'linear_66.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_48.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_48.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_48.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_35@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_48.tmp_0'], Scale=[], Variance=['layer_norm_48.tmp_1'], X=['tmp_35'], Y@GRAD=['layer_norm_48.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_47.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_140.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['layer_norm_47.tmp_2'], Y=['linear_140.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_10.tmp_0@GRAD'], Y@GRAD=['linear_65.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_140.tmp_0@GRAD'], X=['gelu_10.tmp_0'], Y=['linear_65.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_65.w_0', 'linear_65.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_139.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_10.tmp_0@GRAD'], X=['linear_139.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_47.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_64.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_139.tmp_0@GRAD'], X=['layer_norm_47.tmp_2'], Y=['linear_64.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_64.w_0', 'linear_64.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_47.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_47.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_47.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_34@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_47.tmp_0'], Scale=[], Variance=['layer_norm_47.tmp_1'], X=['tmp_34'], Y@GRAD=['layer_norm_47.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_138.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_138.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_43.tmp_0@GRAD'], Y@GRAD=['linear_63.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_138.tmp_0@GRAD'], X=['reshape2_43.tmp_0'], Y=['linear_63.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_63.w_0', 'linear_63.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_43.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_43.tmp_0@GRAD'], XShape=['reshape2_43.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_43.tmp_0@GRAD'], XShape=['transpose_43.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_10.tmp_0@GRAD'], Y@GRAD=['transpose_42.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_10.tmp_0@GRAD'], X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_33@GRAD']} = softmax_grad(inputs={Out=['softmax_10.tmp_0'], Out@GRAD=['softmax_10.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_10.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_33@GRAD'], X=['matmul_10.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_40.tmp_0@GRAD'], Y@GRAD=['transpose_41.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_10.tmp_0@GRAD'], X=['transpose_40.tmp_0'], Y=['transpose_41.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_42.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_42.tmp_0@GRAD'], XShape=['transpose_42.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_137.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_42.tmp_0@GRAD'], XShape=['reshape2_42.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_41.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_41.tmp_0@GRAD'], XShape=['transpose_41.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_136.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_41.tmp_0@GRAD'], XShape=['reshape2_41.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_62.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_137.tmp_0@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_62.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_62.w_0', 'linear_62.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_61.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_136.tmp_0@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_61.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_61.w_0', 'linear_61.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_40.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_40.tmp_0@GRAD'], XShape=['transpose_40.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_135.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_40.tmp_0@GRAD'], XShape=['reshape2_40.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_60.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_135.tmp_0@GRAD'], X=['layer_norm_46.tmp_2'], Y=['linear_60.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_60.w_0', 'linear_60.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_46.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_46.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_46.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_32@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_46.tmp_0'], Scale=[], Variance=['layer_norm_46.tmp_1'], X=['tmp_32'], Y@GRAD=['layer_norm_46.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_45.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_134.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['layer_norm_45.tmp_2'], Y=['linear_134.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_9.tmp_0@GRAD'], Y@GRAD=['linear_59.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_134.tmp_0@GRAD'], X=['gelu_9.tmp_0'], Y=['linear_59.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_59.w_0', 'linear_59.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_133.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_9.tmp_0@GRAD'], X=['linear_133.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_45.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_58.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_133.tmp_0@GRAD'], X=['layer_norm_45.tmp_2'], Y=['linear_58.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_58.w_0', 'linear_58.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_45.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_45.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_45.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_31@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_45.tmp_0'], Scale=[], Variance=['layer_norm_45.tmp_1'], X=['tmp_31'], Y@GRAD=['layer_norm_45.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_132.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_132.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_39.tmp_0@GRAD'], Y@GRAD=['linear_57.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_132.tmp_0@GRAD'], X=['reshape2_39.tmp_0'], Y=['linear_57.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_57.w_0', 'linear_57.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_39.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_39.tmp_0@GRAD'], XShape=['reshape2_39.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_39.tmp_0@GRAD'], XShape=['transpose_39.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_9.tmp_0@GRAD'], Y@GRAD=['transpose_38.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_9.tmp_0@GRAD'], X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_30@GRAD']} = softmax_grad(inputs={Out=['softmax_9.tmp_0'], Out@GRAD=['softmax_9.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_9.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['matmul_9.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_36.tmp_0@GRAD'], Y@GRAD=['transpose_37.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_9.tmp_0@GRAD'], X=['transpose_36.tmp_0'], Y=['transpose_37.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_38.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_38.tmp_0@GRAD'], XShape=['transpose_38.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_131.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_38.tmp_0@GRAD'], XShape=['reshape2_38.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_37.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_37.tmp_0@GRAD'], XShape=['transpose_37.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_130.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_37.tmp_0@GRAD'], XShape=['reshape2_37.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_56.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_131.tmp_0@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_56.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_56.w_0', 'linear_56.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_55.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_130.tmp_0@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_55.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_55.w_0', 'linear_55.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_36.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_36.tmp_0@GRAD'], XShape=['transpose_36.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_129.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_36.tmp_0@GRAD'], XShape=['reshape2_36.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_54.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_129.tmp_0@GRAD'], X=['layer_norm_44.tmp_2'], Y=['linear_54.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_54.w_0', 'linear_54.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_44.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_44.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_44.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_29@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_44.tmp_0'], Scale=[], Variance=['layer_norm_44.tmp_1'], X=['tmp_29'], Y@GRAD=['layer_norm_44.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_43.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_128.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_29@GRAD'], X=['layer_norm_43.tmp_2'], Y=['linear_128.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_8.tmp_0@GRAD'], Y@GRAD=['linear_53.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_128.tmp_0@GRAD'], X=['gelu_8.tmp_0'], Y=['linear_53.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_53.w_0', 'linear_53.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_127.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_8.tmp_0@GRAD'], X=['linear_127.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_43.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_52.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_127.tmp_0@GRAD'], X=['layer_norm_43.tmp_2'], Y=['linear_52.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_52.w_0', 'linear_52.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_43.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_43.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_43.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_28@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_43.tmp_0'], Scale=[], Variance=['layer_norm_43.tmp_1'], X=['tmp_28'], Y@GRAD=['layer_norm_43.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_126.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_126.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_35.tmp_0@GRAD'], Y@GRAD=['linear_51.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_126.tmp_0@GRAD'], X=['reshape2_35.tmp_0'], Y=['linear_51.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_51.w_0', 'linear_51.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_35.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_35.tmp_0@GRAD'], XShape=['reshape2_35.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_35.tmp_0@GRAD'], XShape=['transpose_35.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_8.tmp_0@GRAD'], Y@GRAD=['transpose_34.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_8.tmp_0@GRAD'], X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_27@GRAD']} = softmax_grad(inputs={Out=['softmax_8.tmp_0'], Out@GRAD=['softmax_8.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_8.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['matmul_8.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_32.tmp_0@GRAD'], Y@GRAD=['transpose_33.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_8.tmp_0@GRAD'], X=['transpose_32.tmp_0'], Y=['transpose_33.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_34.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_34.tmp_0@GRAD'], XShape=['transpose_34.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_125.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_34.tmp_0@GRAD'], XShape=['reshape2_34.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_33.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_33.tmp_0@GRAD'], XShape=['transpose_33.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_124.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_33.tmp_0@GRAD'], XShape=['reshape2_33.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_50.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_125.tmp_0@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_50.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_50.w_0', 'linear_50.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_49.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_124.tmp_0@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_49.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_49.w_0', 'linear_49.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_32.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_32.tmp_0@GRAD'], XShape=['transpose_32.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_123.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_32.tmp_0@GRAD'], XShape=['reshape2_32.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_48.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_123.tmp_0@GRAD'], X=['layer_norm_42.tmp_2'], Y=['linear_48.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_48.w_0', 'linear_48.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_42.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_42.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_42.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_26@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_42.tmp_0'], Scale=[], Variance=['layer_norm_42.tmp_1'], X=['tmp_26'], Y@GRAD=['layer_norm_42.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_41.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_122.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['layer_norm_41.tmp_2'], Y=['linear_122.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_7.tmp_0@GRAD'], Y@GRAD=['linear_47.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_122.tmp_0@GRAD'], X=['gelu_7.tmp_0'], Y=['linear_47.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_47.w_0', 'linear_47.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_121.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_7.tmp_0@GRAD'], X=['linear_121.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_41.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_46.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_121.tmp_0@GRAD'], X=['layer_norm_41.tmp_2'], Y=['linear_46.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_46.w_0', 'linear_46.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_41.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_41.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_41.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_25@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_41.tmp_0'], Scale=[], Variance=['layer_norm_41.tmp_1'], X=['tmp_25'], Y@GRAD=['layer_norm_41.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_120.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_25@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_120.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_31.tmp_0@GRAD'], Y@GRAD=['linear_45.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_120.tmp_0@GRAD'], X=['reshape2_31.tmp_0'], Y=['linear_45.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_45.w_0', 'linear_45.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_31.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_31.tmp_0@GRAD'], XShape=['reshape2_31.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_31.tmp_0@GRAD'], XShape=['transpose_31.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_7.tmp_0@GRAD'], Y@GRAD=['transpose_30.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_24@GRAD']} = softmax_grad(inputs={Out=['softmax_7.tmp_0'], Out@GRAD=['softmax_7.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_7.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_24@GRAD'], X=['matmul_7.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_28.tmp_0@GRAD'], Y@GRAD=['transpose_29.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_7.tmp_0@GRAD'], X=['transpose_28.tmp_0'], Y=['transpose_29.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_30.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_30.tmp_0@GRAD'], XShape=['transpose_30.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_119.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_30.tmp_0@GRAD'], XShape=['reshape2_30.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_29.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_29.tmp_0@GRAD'], XShape=['transpose_29.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_118.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_29.tmp_0@GRAD'], XShape=['reshape2_29.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_44.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_119.tmp_0@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_44.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_44.w_0', 'linear_44.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_43.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_118.tmp_0@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_43.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_43.w_0', 'linear_43.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_28.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_28.tmp_0@GRAD'], XShape=['transpose_28.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_117.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_28.tmp_0@GRAD'], XShape=['reshape2_28.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_42.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_117.tmp_0@GRAD'], X=['layer_norm_40.tmp_2'], Y=['linear_42.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_42.w_0', 'linear_42.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_40.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_40.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_40.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_23@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_40.tmp_0'], Scale=[], Variance=['layer_norm_40.tmp_1'], X=['tmp_23'], Y@GRAD=['layer_norm_40.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_39.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_116.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['layer_norm_39.tmp_2'], Y=['linear_116.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_6.tmp_0@GRAD'], Y@GRAD=['linear_41.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_116.tmp_0@GRAD'], X=['gelu_6.tmp_0'], Y=['linear_41.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_41.w_0', 'linear_41.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_115.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_6.tmp_0@GRAD'], X=['linear_115.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_39.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_40.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_115.tmp_0@GRAD'], X=['layer_norm_39.tmp_2'], Y=['linear_40.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_40.w_0', 'linear_40.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_39.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_39.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_39.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_22@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_39.tmp_0'], Scale=[], Variance=['layer_norm_39.tmp_1'], X=['tmp_22'], Y@GRAD=['layer_norm_39.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_114.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_114.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_27.tmp_0@GRAD'], Y@GRAD=['linear_39.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_114.tmp_0@GRAD'], X=['reshape2_27.tmp_0'], Y=['linear_39.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_39.w_0', 'linear_39.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_27.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_27.tmp_0@GRAD'], XShape=['reshape2_27.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_27.tmp_0@GRAD'], XShape=['transpose_27.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_6.tmp_0@GRAD'], Y@GRAD=['transpose_26.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_21@GRAD']} = softmax_grad(inputs={Out=['softmax_6.tmp_0'], Out@GRAD=['softmax_6.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['matmul_6.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_24.tmp_0@GRAD'], Y@GRAD=['transpose_25.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_6.tmp_0@GRAD'], X=['transpose_24.tmp_0'], Y=['transpose_25.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_26.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_26.tmp_0@GRAD'], XShape=['transpose_26.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_113.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_26.tmp_0@GRAD'], XShape=['reshape2_26.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_25.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_25.tmp_0@GRAD'], XShape=['transpose_25.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_112.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_25.tmp_0@GRAD'], XShape=['reshape2_25.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_38.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_113.tmp_0@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_38.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_38.w_0', 'linear_38.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_37.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_112.tmp_0@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_37.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_37.w_0', 'linear_37.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_24.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_24.tmp_0@GRAD'], XShape=['transpose_24.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_111.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_24.tmp_0@GRAD'], XShape=['reshape2_24.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_36.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_111.tmp_0@GRAD'], X=['layer_norm_38.tmp_2'], Y=['linear_36.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_36.w_0', 'linear_36.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_38.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_38.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_38.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_20@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_38.tmp_0'], Scale=[], Variance=['layer_norm_38.tmp_1'], X=['tmp_20'], Y@GRAD=['layer_norm_38.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_37.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_110.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['layer_norm_37.tmp_2'], Y=['linear_110.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_5.tmp_0@GRAD'], Y@GRAD=['linear_35.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_110.tmp_0@GRAD'], X=['gelu_5.tmp_0'], Y=['linear_35.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_35.w_0', 'linear_35.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_109.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_5.tmp_0@GRAD'], X=['linear_109.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_37.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_34.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_109.tmp_0@GRAD'], X=['layer_norm_37.tmp_2'], Y=['linear_34.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_34.w_0', 'linear_34.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_37.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_37.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_37.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_19@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_37.tmp_0'], Scale=[], Variance=['layer_norm_37.tmp_1'], X=['tmp_19'], Y@GRAD=['layer_norm_37.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_108.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_19@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_108.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_23.tmp_0@GRAD'], Y@GRAD=['linear_33.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_108.tmp_0@GRAD'], X=['reshape2_23.tmp_0'], Y=['linear_33.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_33.w_0', 'linear_33.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_23.tmp_0@GRAD'], XShape=['reshape2_23.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_23.tmp_0@GRAD'], XShape=['transpose_23.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_5.tmp_0@GRAD'], Y@GRAD=['transpose_22.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_18@GRAD']} = softmax_grad(inputs={Out=['softmax_5.tmp_0'], Out@GRAD=['softmax_5.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_18@GRAD'], X=['matmul_5.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_20.tmp_0@GRAD'], Y@GRAD=['transpose_21.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_5.tmp_0@GRAD'], X=['transpose_20.tmp_0'], Y=['transpose_21.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_22.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_22.tmp_0@GRAD'], XShape=['transpose_22.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_107.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_22.tmp_0@GRAD'], XShape=['reshape2_22.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_21.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_21.tmp_0@GRAD'], XShape=['transpose_21.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_106.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_21.tmp_0@GRAD'], XShape=['reshape2_21.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_32.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_107.tmp_0@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_32.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_32.w_0', 'linear_32.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_31.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_106.tmp_0@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_31.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_31.w_0', 'linear_31.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_20.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_20.tmp_0@GRAD'], XShape=['transpose_20.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_105.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_20.tmp_0@GRAD'], XShape=['reshape2_20.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_30.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_105.tmp_0@GRAD'], X=['layer_norm_36.tmp_2'], Y=['linear_30.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_30.w_0', 'linear_30.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_36.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_36.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_36.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_17@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_36.tmp_0'], Scale=[], Variance=['layer_norm_36.tmp_1'], X=['tmp_17'], Y@GRAD=['layer_norm_36.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_35.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_104.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['layer_norm_35.tmp_2'], Y=['linear_104.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_4.tmp_0@GRAD'], Y@GRAD=['linear_29.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_104.tmp_0@GRAD'], X=['gelu_4.tmp_0'], Y=['linear_29.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_29.w_0', 'linear_29.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_103.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_4.tmp_0@GRAD'], X=['linear_103.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_35.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_28.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_103.tmp_0@GRAD'], X=['layer_norm_35.tmp_2'], Y=['linear_28.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_28.w_0', 'linear_28.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_35.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_35.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_35.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_16@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_35.tmp_0'], Scale=[], Variance=['layer_norm_35.tmp_1'], X=['tmp_16'], Y@GRAD=['layer_norm_35.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_102.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_16@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_102.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_19.tmp_0@GRAD'], Y@GRAD=['linear_27.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_102.tmp_0@GRAD'], X=['reshape2_19.tmp_0'], Y=['linear_27.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_4.tmp_0@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_15@GRAD']} = softmax_grad(inputs={Out=['softmax_4.tmp_0'], Out@GRAD=['softmax_4.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['matmul_4.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_16.tmp_0@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_4.tmp_0@GRAD'], X=['transpose_16.tmp_0'], Y=['transpose_17.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_18.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_101.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_18.tmp_0@GRAD'], XShape=['reshape2_18.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_100.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_17.tmp_0@GRAD'], XShape=['reshape2_17.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_26.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_101.tmp_0@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_26.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_25.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_100.tmp_0@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_25.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_99.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_16.tmp_0@GRAD'], XShape=['reshape2_16.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_24.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_99.tmp_0@GRAD'], X=['layer_norm_34.tmp_2'], Y=['linear_24.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_34.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_34.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_34.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_14@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_34.tmp_0'], Scale=[], Variance=['layer_norm_34.tmp_1'], X=['tmp_14'], Y@GRAD=['layer_norm_34.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_33.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_98.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_14@GRAD'], X=['layer_norm_33.tmp_2'], Y=['linear_98.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_3.tmp_0@GRAD'], Y@GRAD=['linear_23.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_98.tmp_0@GRAD'], X=['gelu_3.tmp_0'], Y=['linear_23.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_97.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_3.tmp_0@GRAD'], X=['linear_97.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_33.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_22.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_97.tmp_0@GRAD'], X=['layer_norm_33.tmp_2'], Y=['linear_22.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_33.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_33.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_33.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_13@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_33.tmp_0'], Scale=[], Variance=['layer_norm_33.tmp_1'], X=['tmp_13'], Y@GRAD=['layer_norm_33.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_96.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_96.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_15.tmp_0@GRAD'], Y@GRAD=['linear_21.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_96.tmp_0@GRAD'], X=['reshape2_15.tmp_0'], Y=['linear_21.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_3.tmp_0@GRAD'], Y@GRAD=['transpose_14.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_12@GRAD']} = softmax_grad(inputs={Out=['softmax_3.tmp_0'], Out@GRAD=['softmax_3.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['matmul_3.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_12.tmp_0@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_3.tmp_0@GRAD'], X=['transpose_12.tmp_0'], Y=['transpose_13.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_14.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_95.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_14.tmp_0@GRAD'], XShape=['reshape2_14.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_13.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_94.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_13.tmp_0@GRAD'], XShape=['reshape2_13.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_20.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_95.tmp_0@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_20.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_19.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_94.tmp_0@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_19.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_93.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_18.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_93.tmp_0@GRAD'], X=['layer_norm_32.tmp_2'], Y=['linear_18.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_32.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_32.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_32.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_11@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_32.tmp_0'], Scale=[], Variance=['layer_norm_32.tmp_1'], X=['tmp_11'], Y@GRAD=['layer_norm_32.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_31.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_92.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_11@GRAD'], X=['layer_norm_31.tmp_2'], Y=['linear_92.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_2.tmp_0@GRAD'], Y@GRAD=['linear_17.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_92.tmp_0@GRAD'], X=['gelu_2.tmp_0'], Y=['linear_17.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_91.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_2.tmp_0@GRAD'], X=['linear_91.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_31.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_16.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_91.tmp_0@GRAD'], X=['layer_norm_31.tmp_2'], Y=['linear_16.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_31.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_31.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_31.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_10@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_31.tmp_0'], Scale=[], Variance=['layer_norm_31.tmp_1'], X=['tmp_10'], Y@GRAD=['layer_norm_31.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_90.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_90.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_11.tmp_0@GRAD'], Y@GRAD=['linear_15.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_90.tmp_0@GRAD'], X=['reshape2_11.tmp_0'], Y=['linear_15.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD'], Y@GRAD=['transpose_10.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_9@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_0'], Out@GRAD=['softmax_2.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['matmul_2.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_8.tmp_0@GRAD'], Y@GRAD=['transpose_9.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_2.tmp_0@GRAD'], X=['transpose_8.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_89.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_88.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_14.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_89.tmp_0@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_14.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_13.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_88.tmp_0@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_13.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_87.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_8.tmp_0@GRAD'], XShape=['reshape2_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_12.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_87.tmp_0@GRAD'], X=['layer_norm_30.tmp_2'], Y=['linear_12.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_30.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_30.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_30.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_8@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_30.tmp_0'], Scale=[], Variance=['layer_norm_30.tmp_1'], X=['tmp_8'], Y@GRAD=['layer_norm_30.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_29.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_86.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['layer_norm_29.tmp_2'], Y=['linear_86.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_1.tmp_0@GRAD'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_86.tmp_0@GRAD'], X=['gelu_1.tmp_0'], Y=['linear_11.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_85.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_1.tmp_0@GRAD'], X=['linear_85.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_29.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_85.tmp_0@GRAD'], X=['layer_norm_29.tmp_2'], Y=['linear_10.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_29.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_29.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_29.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_7@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_29.tmp_0'], Scale=[], Variance=['layer_norm_29.tmp_1'], X=['tmp_7'], Y@GRAD=['layer_norm_29.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_84.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_7@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_84.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_7.tmp_0@GRAD'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_84.tmp_0@GRAD'], X=['reshape2_7.tmp_0'], Y=['linear_9.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_6@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_0'], Out@GRAD=['softmax_1.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['matmul_1.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_4.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_1.tmp_0@GRAD'], X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_83.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_82.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_83.tmp_0@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_8.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_82.tmp_0@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_7.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_81.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_81.tmp_0@GRAD'], X=['layer_norm_28.tmp_2'], Y=['linear_6.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_28.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_28.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_28.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_5@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_28.tmp_0'], Scale=[], Variance=['layer_norm_28.tmp_1'], X=['tmp_5'], Y@GRAD=['layer_norm_28.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_27.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_80.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['layer_norm_27.tmp_2'], Y=['linear_80.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_0.tmp_0@GRAD'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_80.tmp_0@GRAD'], X=['gelu_0.tmp_0'], Y=['linear_5.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_79.tmp_0@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_0.tmp_0@GRAD'], X=['linear_79.tmp_0']}, approximate = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['layer_norm_27.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_79.tmp_0@GRAD'], X=['layer_norm_27.tmp_2'], Y=['linear_4.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_27.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_27.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_27.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=[], Scale@GRAD=[], X@GRAD=['tmp_4@GRAD']} = layer_norm_grad(inputs={Bias=[], Mean=['layer_norm_27.tmp_0'], Scale=[], Variance=['layer_norm_27.tmp_1'], X=['tmp_4'], Y@GRAD=['layer_norm_27.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_78.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_78.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_3.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_78.tmp_0@GRAD'], X=['reshape2_3.tmp_0'], Y=['linear_3.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False)
    {X@GRAD=['tmp_3@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_0'], Out@GRAD=['softmax_0.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['matmul_0.tmp_0'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_0.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_0.tmp_0@GRAD'], X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_77.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_76.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_77.tmp_0@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_2.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_76.tmp_0@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_1.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['linear_75.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['linear_75.tmp_0@GRAD'], X=['layer_norm_26.tmp_2'], Y=['linear_0.w_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['layer_norm_26.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_26.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_26.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['layer_norm_0.b_0@GRAD'], Scale@GRAD=['layer_norm_0.w_0@GRAD'], X@GRAD=['tmp_2@GRAD']} = layer_norm_grad(inputs={Bias=['layer_norm_0.b_0'], Mean=['layer_norm_26.tmp_0'], Scale=['layer_norm_0.w_0'], Variance=['layer_norm_26.tmp_1'], X=['tmp_2'], Y@GRAD=['layer_norm_26.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD', 'layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'], use_mkldnn = False)
    {X@GRAD=['tmp_1@GRAD'], Y@GRAD=['embedding_5.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['tmp_1'], Y=['embedding_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['embedding_3.tmp_0@GRAD'], Y@GRAD=['embedding_4.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['embedding_3.tmp_0'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {W@GRAD=['embedding_2.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['segment_ids'], Out@GRAD=['embedding_5.tmp_0@GRAD'], W=['embedding_2.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_2.w_0', 'embedding_2.w_0@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['embedding_1.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['tmp_0'], Out@GRAD=['embedding_4.tmp_0@GRAD'], W=['embedding_1.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['embedding_0.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['input_ids'], Out@GRAD=['embedding_3.tmp_0@GRAD'], W=['embedding_0.w_0']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Beta1PowOut=['bert_lm_prediction_head_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['bert_lm_prediction_head_0.b_0_beta2_pow_acc_0'], Moment1Out=['bert_lm_prediction_head_0.b_0_moment1_0'], Moment2Out=['bert_lm_prediction_head_0.b_0_moment2_0'], ParamOut=['bert_lm_prediction_head_0.b_0']} = adam(inputs={Beta1Pow=['bert_lm_prediction_head_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['bert_lm_prediction_head_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['bert_lm_prediction_head_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['bert_lm_prediction_head_0.b_0_moment1_0'], Moment2=['bert_lm_prediction_head_0.b_0_moment2_0'], Param=['bert_lm_prediction_head_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.b_0', 'bert_lm_prediction_head_0.b_0@GRAD'])
    {Out=['tmp_42']} = scale(inputs={ScaleTensor=[], X=['learning_rate_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], scale = 0.009999999776482582)
    {Out=['tmp_43']} = scale(inputs={ScaleTensor=[], X=['tmp_42']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], scale = -1.0)
    {Out=['tmp_44']} = elementwise_mul(inputs={X=['bert_lm_prediction_head_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['bert_lm_prediction_head_0.w_0']} = assign(inputs={X=['tmp_44']}, op_device = , op_namescope = /optimizer_1/weight decay/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'])
    {Beta1PowOut=['bert_lm_prediction_head_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['bert_lm_prediction_head_0.w_0_beta2_pow_acc_0'], Moment1Out=['bert_lm_prediction_head_0.w_0_moment1_0'], Moment2Out=['bert_lm_prediction_head_0.w_0_moment2_0'], ParamOut=['bert_lm_prediction_head_0.w_0']} = adam(inputs={Beta1Pow=['bert_lm_prediction_head_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['bert_lm_prediction_head_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['bert_lm_prediction_head_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['bert_lm_prediction_head_0.w_0_moment1_0'], Moment2=['bert_lm_prediction_head_0.w_0_moment2_0'], Param=['bert_lm_prediction_head_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['bert_lm_prediction_head_0.w_0', 'bert_lm_prediction_head_0.w_0@GRAD'])
    {Out=['tmp_45']} = elementwise_mul(inputs={X=['embedding_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_2/weight decay/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_0.w_0']} = assign(inputs={X=['tmp_45']}, op_device = , op_namescope = /optimizer_2/weight decay/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'])
    {Beta1PowOut=['embedding_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_0.w_0_beta2_pow_acc_0'], Moment1Out=['embedding_0.w_0_moment1_0'], Moment2Out=['embedding_0.w_0_moment2_0'], ParamOut=['embedding_0.w_0']} = adam(inputs={Beta1Pow=['embedding_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['embedding_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['embedding_0.w_0_moment1_0'], Moment2=['embedding_0.w_0_moment2_0'], Param=['embedding_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'])
    {Out=['tmp_46']} = elementwise_mul(inputs={X=['embedding_1.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_3/weight decay/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_1.w_0']} = assign(inputs={X=['tmp_46']}, op_device = , op_namescope = /optimizer_3/weight decay/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'])
    {Beta1PowOut=['embedding_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_1.w_0_beta2_pow_acc_0'], Moment1Out=['embedding_1.w_0_moment1_0'], Moment2Out=['embedding_1.w_0_moment2_0'], ParamOut=['embedding_1.w_0']} = adam(inputs={Beta1Pow=['embedding_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['embedding_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['embedding_1.w_0_moment1_0'], Moment2=['embedding_1.w_0_moment2_0'], Param=['embedding_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'])
    {Out=['tmp_47']} = elementwise_mul(inputs={X=['embedding_2.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_4/weight decay/, op_role = 2, op_role_var = ['embedding_2.w_0', 'embedding_2.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['embedding_2.w_0']} = assign(inputs={X=['tmp_47']}, op_device = , op_namescope = /optimizer_4/weight decay/, op_role = 2, op_role_var = ['embedding_2.w_0', 'embedding_2.w_0@GRAD'])
    {Beta1PowOut=['embedding_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_2.w_0_beta2_pow_acc_0'], Moment1Out=['embedding_2.w_0_moment1_0'], Moment2Out=['embedding_2.w_0_moment2_0'], ParamOut=['embedding_2.w_0']} = adam(inputs={Beta1Pow=['embedding_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['embedding_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['embedding_2.w_0_moment1_0'], Moment2=['embedding_2.w_0_moment2_0'], Param=['embedding_2.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['embedding_2.w_0', 'embedding_2.w_0@GRAD'])
    {Beta1PowOut=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.b_0_moment1_0'], Moment2Out=['layer_norm_0.b_0_moment2_0'], ParamOut=['layer_norm_0.b_0']} = adam(inputs={Beta1Pow=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['layer_norm_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.b_0_moment1_0'], Moment2=['layer_norm_0.b_0_moment2_0'], Param=['layer_norm_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD'])
    {Beta1PowOut=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.w_0_moment1_0'], Moment2Out=['layer_norm_0.w_0_moment2_0'], ParamOut=['layer_norm_0.w_0']} = adam(inputs={Beta1Pow=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['layer_norm_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.w_0_moment1_0'], Moment2=['layer_norm_0.w_0_moment2_0'], Param=['layer_norm_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'])
    {Out=['tmp_48']} = elementwise_mul(inputs={X=['linear_0.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_7/weight decay/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_0.w_0']} = assign(inputs={X=['tmp_48']}, op_device = , op_namescope = /optimizer_7/weight decay/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'])
    {Beta1PowOut=['linear_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_beta2_pow_acc_0'], Moment1Out=['linear_0.w_0_moment1_0'], Moment2Out=['linear_0.w_0_moment2_0'], ParamOut=['linear_0.w_0']} = adam(inputs={Beta1Pow=['linear_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_0.w_0_moment1_0'], Moment2=['linear_0.w_0_moment2_0'], Param=['linear_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'])
    {Out=['tmp_49']} = elementwise_mul(inputs={X=['linear_1.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_8/weight decay/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_1.w_0']} = assign(inputs={X=['tmp_49']}, op_device = , op_namescope = /optimizer_8/weight decay/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'])
    {Beta1PowOut=['linear_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_beta2_pow_acc_0'], Moment1Out=['linear_1.w_0_moment1_0'], Moment2Out=['linear_1.w_0_moment2_0'], ParamOut=['linear_1.w_0']} = adam(inputs={Beta1Pow=['linear_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_1.w_0_moment1_0'], Moment2=['linear_1.w_0_moment2_0'], Param=['linear_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'])
    {Out=['tmp_50']} = elementwise_mul(inputs={X=['linear_10.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_9/weight decay/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_10.w_0']} = assign(inputs={X=['tmp_50']}, op_device = , op_namescope = /optimizer_9/weight decay/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'])
    {Beta1PowOut=['linear_10.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.w_0_beta2_pow_acc_0'], Moment1Out=['linear_10.w_0_moment1_0'], Moment2Out=['linear_10.w_0_moment2_0'], ParamOut=['linear_10.w_0']} = adam(inputs={Beta1Pow=['linear_10.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_10.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_10.w_0_moment1_0'], Moment2=['linear_10.w_0_moment2_0'], Param=['linear_10.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'])
    {Out=['tmp_51']} = elementwise_mul(inputs={X=['linear_11.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_10/weight decay/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_11.w_0']} = assign(inputs={X=['tmp_51']}, op_device = , op_namescope = /optimizer_10/weight decay/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'])
    {Beta1PowOut=['linear_11.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.w_0_beta2_pow_acc_0'], Moment1Out=['linear_11.w_0_moment1_0'], Moment2Out=['linear_11.w_0_moment2_0'], ParamOut=['linear_11.w_0']} = adam(inputs={Beta1Pow=['linear_11.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_11.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_11.w_0_moment1_0'], Moment2=['linear_11.w_0_moment2_0'], Param=['linear_11.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_10/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'])
    {Out=['tmp_52']} = elementwise_mul(inputs={X=['linear_12.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_11/weight decay/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_12.w_0']} = assign(inputs={X=['tmp_52']}, op_device = , op_namescope = /optimizer_11/weight decay/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'])
    {Beta1PowOut=['linear_12.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_12.w_0_beta2_pow_acc_0'], Moment1Out=['linear_12.w_0_moment1_0'], Moment2Out=['linear_12.w_0_moment2_0'], ParamOut=['linear_12.w_0']} = adam(inputs={Beta1Pow=['linear_12.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_12.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_12.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_12.w_0_moment1_0'], Moment2=['linear_12.w_0_moment2_0'], Param=['linear_12.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_11/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'])
    {Out=['tmp_53']} = elementwise_mul(inputs={X=['linear_13.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_12/weight decay/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_13.w_0']} = assign(inputs={X=['tmp_53']}, op_device = , op_namescope = /optimizer_12/weight decay/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'])
    {Beta1PowOut=['linear_13.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_13.w_0_beta2_pow_acc_0'], Moment1Out=['linear_13.w_0_moment1_0'], Moment2Out=['linear_13.w_0_moment2_0'], ParamOut=['linear_13.w_0']} = adam(inputs={Beta1Pow=['linear_13.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_13.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_13.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_13.w_0_moment1_0'], Moment2=['linear_13.w_0_moment2_0'], Param=['linear_13.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_12/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'])
    {Out=['tmp_54']} = elementwise_mul(inputs={X=['linear_14.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_13/weight decay/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_14.w_0']} = assign(inputs={X=['tmp_54']}, op_device = , op_namescope = /optimizer_13/weight decay/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'])
    {Beta1PowOut=['linear_14.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_14.w_0_beta2_pow_acc_0'], Moment1Out=['linear_14.w_0_moment1_0'], Moment2Out=['linear_14.w_0_moment2_0'], ParamOut=['linear_14.w_0']} = adam(inputs={Beta1Pow=['linear_14.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_14.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_14.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_14.w_0_moment1_0'], Moment2=['linear_14.w_0_moment2_0'], Param=['linear_14.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_13/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'])
    {Out=['tmp_55']} = elementwise_mul(inputs={X=['linear_15.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_14/weight decay/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_15.w_0']} = assign(inputs={X=['tmp_55']}, op_device = , op_namescope = /optimizer_14/weight decay/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'])
    {Beta1PowOut=['linear_15.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_15.w_0_beta2_pow_acc_0'], Moment1Out=['linear_15.w_0_moment1_0'], Moment2Out=['linear_15.w_0_moment2_0'], ParamOut=['linear_15.w_0']} = adam(inputs={Beta1Pow=['linear_15.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_15.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_15.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_15.w_0_moment1_0'], Moment2=['linear_15.w_0_moment2_0'], Param=['linear_15.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_14/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'])
    {Out=['tmp_56']} = elementwise_mul(inputs={X=['linear_16.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_15/weight decay/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_16.w_0']} = assign(inputs={X=['tmp_56']}, op_device = , op_namescope = /optimizer_15/weight decay/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'])
    {Beta1PowOut=['linear_16.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_16.w_0_beta2_pow_acc_0'], Moment1Out=['linear_16.w_0_moment1_0'], Moment2Out=['linear_16.w_0_moment2_0'], ParamOut=['linear_16.w_0']} = adam(inputs={Beta1Pow=['linear_16.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_16.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_16.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_16.w_0_moment1_0'], Moment2=['linear_16.w_0_moment2_0'], Param=['linear_16.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_15/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'])
    {Out=['tmp_57']} = elementwise_mul(inputs={X=['linear_17.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_16/weight decay/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_17.w_0']} = assign(inputs={X=['tmp_57']}, op_device = , op_namescope = /optimizer_16/weight decay/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'])
    {Beta1PowOut=['linear_17.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_17.w_0_beta2_pow_acc_0'], Moment1Out=['linear_17.w_0_moment1_0'], Moment2Out=['linear_17.w_0_moment2_0'], ParamOut=['linear_17.w_0']} = adam(inputs={Beta1Pow=['linear_17.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_17.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_17.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_17.w_0_moment1_0'], Moment2=['linear_17.w_0_moment2_0'], Param=['linear_17.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_16/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'])
    {Out=['tmp_58']} = elementwise_mul(inputs={X=['linear_18.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_17/weight decay/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_18.w_0']} = assign(inputs={X=['tmp_58']}, op_device = , op_namescope = /optimizer_17/weight decay/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'])
    {Beta1PowOut=['linear_18.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_18.w_0_beta2_pow_acc_0'], Moment1Out=['linear_18.w_0_moment1_0'], Moment2Out=['linear_18.w_0_moment2_0'], ParamOut=['linear_18.w_0']} = adam(inputs={Beta1Pow=['linear_18.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_18.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_18.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_18.w_0_moment1_0'], Moment2=['linear_18.w_0_moment2_0'], Param=['linear_18.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_17/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'])
    {Out=['tmp_59']} = elementwise_mul(inputs={X=['linear_19.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_18/weight decay/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_19.w_0']} = assign(inputs={X=['tmp_59']}, op_device = , op_namescope = /optimizer_18/weight decay/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'])
    {Beta1PowOut=['linear_19.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_19.w_0_beta2_pow_acc_0'], Moment1Out=['linear_19.w_0_moment1_0'], Moment2Out=['linear_19.w_0_moment2_0'], ParamOut=['linear_19.w_0']} = adam(inputs={Beta1Pow=['linear_19.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_19.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_19.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_19.w_0_moment1_0'], Moment2=['linear_19.w_0_moment2_0'], Param=['linear_19.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_18/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'])
    {Out=['tmp_60']} = elementwise_mul(inputs={X=['linear_2.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_19/weight decay/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_2.w_0']} = assign(inputs={X=['tmp_60']}, op_device = , op_namescope = /optimizer_19/weight decay/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'])
    {Beta1PowOut=['linear_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_beta2_pow_acc_0'], Moment1Out=['linear_2.w_0_moment1_0'], Moment2Out=['linear_2.w_0_moment2_0'], ParamOut=['linear_2.w_0']} = adam(inputs={Beta1Pow=['linear_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_2.w_0_moment1_0'], Moment2=['linear_2.w_0_moment2_0'], Param=['linear_2.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_19/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'])
    {Out=['tmp_61']} = elementwise_mul(inputs={X=['linear_20.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_20/weight decay/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_20.w_0']} = assign(inputs={X=['tmp_61']}, op_device = , op_namescope = /optimizer_20/weight decay/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'])
    {Beta1PowOut=['linear_20.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_20.w_0_beta2_pow_acc_0'], Moment1Out=['linear_20.w_0_moment1_0'], Moment2Out=['linear_20.w_0_moment2_0'], ParamOut=['linear_20.w_0']} = adam(inputs={Beta1Pow=['linear_20.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_20.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_20.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_20.w_0_moment1_0'], Moment2=['linear_20.w_0_moment2_0'], Param=['linear_20.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'])
    {Out=['tmp_62']} = elementwise_mul(inputs={X=['linear_21.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_21/weight decay/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_21.w_0']} = assign(inputs={X=['tmp_62']}, op_device = , op_namescope = /optimizer_21/weight decay/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'])
    {Beta1PowOut=['linear_21.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_21.w_0_beta2_pow_acc_0'], Moment1Out=['linear_21.w_0_moment1_0'], Moment2Out=['linear_21.w_0_moment2_0'], ParamOut=['linear_21.w_0']} = adam(inputs={Beta1Pow=['linear_21.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_21.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_21.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_21.w_0_moment1_0'], Moment2=['linear_21.w_0_moment2_0'], Param=['linear_21.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_21/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'])
    {Out=['tmp_63']} = elementwise_mul(inputs={X=['linear_22.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_22/weight decay/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_22.w_0']} = assign(inputs={X=['tmp_63']}, op_device = , op_namescope = /optimizer_22/weight decay/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'])
    {Beta1PowOut=['linear_22.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_22.w_0_beta2_pow_acc_0'], Moment1Out=['linear_22.w_0_moment1_0'], Moment2Out=['linear_22.w_0_moment2_0'], ParamOut=['linear_22.w_0']} = adam(inputs={Beta1Pow=['linear_22.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_22.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_22.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_22.w_0_moment1_0'], Moment2=['linear_22.w_0_moment2_0'], Param=['linear_22.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'])
    {Out=['tmp_64']} = elementwise_mul(inputs={X=['linear_23.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_23/weight decay/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_23.w_0']} = assign(inputs={X=['tmp_64']}, op_device = , op_namescope = /optimizer_23/weight decay/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'])
    {Beta1PowOut=['linear_23.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_23.w_0_beta2_pow_acc_0'], Moment1Out=['linear_23.w_0_moment1_0'], Moment2Out=['linear_23.w_0_moment2_0'], ParamOut=['linear_23.w_0']} = adam(inputs={Beta1Pow=['linear_23.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_23.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_23.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_23.w_0_moment1_0'], Moment2=['linear_23.w_0_moment2_0'], Param=['linear_23.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'])
    {Out=['tmp_65']} = elementwise_mul(inputs={X=['linear_24.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_24/weight decay/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_24.w_0']} = assign(inputs={X=['tmp_65']}, op_device = , op_namescope = /optimizer_24/weight decay/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'])
    {Beta1PowOut=['linear_24.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_24.w_0_beta2_pow_acc_0'], Moment1Out=['linear_24.w_0_moment1_0'], Moment2Out=['linear_24.w_0_moment2_0'], ParamOut=['linear_24.w_0']} = adam(inputs={Beta1Pow=['linear_24.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_24.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_24.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_24.w_0_moment1_0'], Moment2=['linear_24.w_0_moment2_0'], Param=['linear_24.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_24/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'])
    {Out=['tmp_66']} = elementwise_mul(inputs={X=['linear_25.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_25/weight decay/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_25.w_0']} = assign(inputs={X=['tmp_66']}, op_device = , op_namescope = /optimizer_25/weight decay/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'])
    {Beta1PowOut=['linear_25.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_25.w_0_beta2_pow_acc_0'], Moment1Out=['linear_25.w_0_moment1_0'], Moment2Out=['linear_25.w_0_moment2_0'], ParamOut=['linear_25.w_0']} = adam(inputs={Beta1Pow=['linear_25.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_25.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_25.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_25.w_0_moment1_0'], Moment2=['linear_25.w_0_moment2_0'], Param=['linear_25.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'])
    {Out=['tmp_67']} = elementwise_mul(inputs={X=['linear_26.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_26/weight decay/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_26.w_0']} = assign(inputs={X=['tmp_67']}, op_device = , op_namescope = /optimizer_26/weight decay/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'])
    {Beta1PowOut=['linear_26.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_26.w_0_beta2_pow_acc_0'], Moment1Out=['linear_26.w_0_moment1_0'], Moment2Out=['linear_26.w_0_moment2_0'], ParamOut=['linear_26.w_0']} = adam(inputs={Beta1Pow=['linear_26.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_26.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_26.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_26.w_0_moment1_0'], Moment2=['linear_26.w_0_moment2_0'], Param=['linear_26.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'])
    {Out=['tmp_68']} = elementwise_mul(inputs={X=['linear_27.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_27/weight decay/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_27.w_0']} = assign(inputs={X=['tmp_68']}, op_device = , op_namescope = /optimizer_27/weight decay/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'])
    {Beta1PowOut=['linear_27.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_27.w_0_beta2_pow_acc_0'], Moment1Out=['linear_27.w_0_moment1_0'], Moment2Out=['linear_27.w_0_moment2_0'], ParamOut=['linear_27.w_0']} = adam(inputs={Beta1Pow=['linear_27.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_27.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_27.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_27.w_0_moment1_0'], Moment2=['linear_27.w_0_moment2_0'], Param=['linear_27.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_27/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'])
    {Out=['tmp_69']} = elementwise_mul(inputs={X=['linear_28.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_28/weight decay/, op_role = 2, op_role_var = ['linear_28.w_0', 'linear_28.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_28.w_0']} = assign(inputs={X=['tmp_69']}, op_device = , op_namescope = /optimizer_28/weight decay/, op_role = 2, op_role_var = ['linear_28.w_0', 'linear_28.w_0@GRAD'])
    {Beta1PowOut=['linear_28.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_28.w_0_beta2_pow_acc_0'], Moment1Out=['linear_28.w_0_moment1_0'], Moment2Out=['linear_28.w_0_moment2_0'], ParamOut=['linear_28.w_0']} = adam(inputs={Beta1Pow=['linear_28.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_28.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_28.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_28.w_0_moment1_0'], Moment2=['linear_28.w_0_moment2_0'], Param=['linear_28.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_28/, op_role = 2, op_role_var = ['linear_28.w_0', 'linear_28.w_0@GRAD'])
    {Out=['tmp_70']} = elementwise_mul(inputs={X=['linear_29.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_29/weight decay/, op_role = 2, op_role_var = ['linear_29.w_0', 'linear_29.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_29.w_0']} = assign(inputs={X=['tmp_70']}, op_device = , op_namescope = /optimizer_29/weight decay/, op_role = 2, op_role_var = ['linear_29.w_0', 'linear_29.w_0@GRAD'])
    {Beta1PowOut=['linear_29.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_29.w_0_beta2_pow_acc_0'], Moment1Out=['linear_29.w_0_moment1_0'], Moment2Out=['linear_29.w_0_moment2_0'], ParamOut=['linear_29.w_0']} = adam(inputs={Beta1Pow=['linear_29.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_29.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_29.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_29.w_0_moment1_0'], Moment2=['linear_29.w_0_moment2_0'], Param=['linear_29.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_29/, op_role = 2, op_role_var = ['linear_29.w_0', 'linear_29.w_0@GRAD'])
    {Out=['tmp_71']} = elementwise_mul(inputs={X=['linear_3.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_30/weight decay/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_3.w_0']} = assign(inputs={X=['tmp_71']}, op_device = , op_namescope = /optimizer_30/weight decay/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'])
    {Beta1PowOut=['linear_3.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.w_0_beta2_pow_acc_0'], Moment1Out=['linear_3.w_0_moment1_0'], Moment2Out=['linear_3.w_0_moment2_0'], ParamOut=['linear_3.w_0']} = adam(inputs={Beta1Pow=['linear_3.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_3.w_0_moment1_0'], Moment2=['linear_3.w_0_moment2_0'], Param=['linear_3.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_30/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'])
    {Out=['tmp_72']} = elementwise_mul(inputs={X=['linear_30.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_31/weight decay/, op_role = 2, op_role_var = ['linear_30.w_0', 'linear_30.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_30.w_0']} = assign(inputs={X=['tmp_72']}, op_device = , op_namescope = /optimizer_31/weight decay/, op_role = 2, op_role_var = ['linear_30.w_0', 'linear_30.w_0@GRAD'])
    {Beta1PowOut=['linear_30.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_30.w_0_beta2_pow_acc_0'], Moment1Out=['linear_30.w_0_moment1_0'], Moment2Out=['linear_30.w_0_moment2_0'], ParamOut=['linear_30.w_0']} = adam(inputs={Beta1Pow=['linear_30.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_30.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_30.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_30.w_0_moment1_0'], Moment2=['linear_30.w_0_moment2_0'], Param=['linear_30.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['linear_30.w_0', 'linear_30.w_0@GRAD'])
    {Out=['tmp_73']} = elementwise_mul(inputs={X=['linear_31.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_32/weight decay/, op_role = 2, op_role_var = ['linear_31.w_0', 'linear_31.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_31.w_0']} = assign(inputs={X=['tmp_73']}, op_device = , op_namescope = /optimizer_32/weight decay/, op_role = 2, op_role_var = ['linear_31.w_0', 'linear_31.w_0@GRAD'])
    {Beta1PowOut=['linear_31.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_31.w_0_beta2_pow_acc_0'], Moment1Out=['linear_31.w_0_moment1_0'], Moment2Out=['linear_31.w_0_moment2_0'], ParamOut=['linear_31.w_0']} = adam(inputs={Beta1Pow=['linear_31.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_31.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_31.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_31.w_0_moment1_0'], Moment2=['linear_31.w_0_moment2_0'], Param=['linear_31.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_32/, op_role = 2, op_role_var = ['linear_31.w_0', 'linear_31.w_0@GRAD'])
    {Out=['tmp_74']} = elementwise_mul(inputs={X=['linear_32.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_33/weight decay/, op_role = 2, op_role_var = ['linear_32.w_0', 'linear_32.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_32.w_0']} = assign(inputs={X=['tmp_74']}, op_device = , op_namescope = /optimizer_33/weight decay/, op_role = 2, op_role_var = ['linear_32.w_0', 'linear_32.w_0@GRAD'])
    {Beta1PowOut=['linear_32.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_32.w_0_beta2_pow_acc_0'], Moment1Out=['linear_32.w_0_moment1_0'], Moment2Out=['linear_32.w_0_moment2_0'], ParamOut=['linear_32.w_0']} = adam(inputs={Beta1Pow=['linear_32.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_32.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_32.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_32.w_0_moment1_0'], Moment2=['linear_32.w_0_moment2_0'], Param=['linear_32.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['linear_32.w_0', 'linear_32.w_0@GRAD'])
    {Out=['tmp_75']} = elementwise_mul(inputs={X=['linear_33.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_34/weight decay/, op_role = 2, op_role_var = ['linear_33.w_0', 'linear_33.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_33.w_0']} = assign(inputs={X=['tmp_75']}, op_device = , op_namescope = /optimizer_34/weight decay/, op_role = 2, op_role_var = ['linear_33.w_0', 'linear_33.w_0@GRAD'])
    {Beta1PowOut=['linear_33.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_33.w_0_beta2_pow_acc_0'], Moment1Out=['linear_33.w_0_moment1_0'], Moment2Out=['linear_33.w_0_moment2_0'], ParamOut=['linear_33.w_0']} = adam(inputs={Beta1Pow=['linear_33.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_33.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_33.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_33.w_0_moment1_0'], Moment2=['linear_33.w_0_moment2_0'], Param=['linear_33.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_34/, op_role = 2, op_role_var = ['linear_33.w_0', 'linear_33.w_0@GRAD'])
    {Out=['tmp_76']} = elementwise_mul(inputs={X=['linear_34.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_35/weight decay/, op_role = 2, op_role_var = ['linear_34.w_0', 'linear_34.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_34.w_0']} = assign(inputs={X=['tmp_76']}, op_device = , op_namescope = /optimizer_35/weight decay/, op_role = 2, op_role_var = ['linear_34.w_0', 'linear_34.w_0@GRAD'])
    {Beta1PowOut=['linear_34.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_34.w_0_beta2_pow_acc_0'], Moment1Out=['linear_34.w_0_moment1_0'], Moment2Out=['linear_34.w_0_moment2_0'], ParamOut=['linear_34.w_0']} = adam(inputs={Beta1Pow=['linear_34.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_34.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_34.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_34.w_0_moment1_0'], Moment2=['linear_34.w_0_moment2_0'], Param=['linear_34.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_35/, op_role = 2, op_role_var = ['linear_34.w_0', 'linear_34.w_0@GRAD'])
    {Out=['tmp_77']} = elementwise_mul(inputs={X=['linear_35.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_36/weight decay/, op_role = 2, op_role_var = ['linear_35.w_0', 'linear_35.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_35.w_0']} = assign(inputs={X=['tmp_77']}, op_device = , op_namescope = /optimizer_36/weight decay/, op_role = 2, op_role_var = ['linear_35.w_0', 'linear_35.w_0@GRAD'])
    {Beta1PowOut=['linear_35.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_35.w_0_beta2_pow_acc_0'], Moment1Out=['linear_35.w_0_moment1_0'], Moment2Out=['linear_35.w_0_moment2_0'], ParamOut=['linear_35.w_0']} = adam(inputs={Beta1Pow=['linear_35.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_35.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_35.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_35.w_0_moment1_0'], Moment2=['linear_35.w_0_moment2_0'], Param=['linear_35.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_36/, op_role = 2, op_role_var = ['linear_35.w_0', 'linear_35.w_0@GRAD'])
    {Out=['tmp_78']} = elementwise_mul(inputs={X=['linear_36.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_37/weight decay/, op_role = 2, op_role_var = ['linear_36.w_0', 'linear_36.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_36.w_0']} = assign(inputs={X=['tmp_78']}, op_device = , op_namescope = /optimizer_37/weight decay/, op_role = 2, op_role_var = ['linear_36.w_0', 'linear_36.w_0@GRAD'])
    {Beta1PowOut=['linear_36.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_36.w_0_beta2_pow_acc_0'], Moment1Out=['linear_36.w_0_moment1_0'], Moment2Out=['linear_36.w_0_moment2_0'], ParamOut=['linear_36.w_0']} = adam(inputs={Beta1Pow=['linear_36.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_36.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_36.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_36.w_0_moment1_0'], Moment2=['linear_36.w_0_moment2_0'], Param=['linear_36.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_37/, op_role = 2, op_role_var = ['linear_36.w_0', 'linear_36.w_0@GRAD'])
    {Out=['tmp_79']} = elementwise_mul(inputs={X=['linear_37.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_38/weight decay/, op_role = 2, op_role_var = ['linear_37.w_0', 'linear_37.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_37.w_0']} = assign(inputs={X=['tmp_79']}, op_device = , op_namescope = /optimizer_38/weight decay/, op_role = 2, op_role_var = ['linear_37.w_0', 'linear_37.w_0@GRAD'])
    {Beta1PowOut=['linear_37.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_37.w_0_beta2_pow_acc_0'], Moment1Out=['linear_37.w_0_moment1_0'], Moment2Out=['linear_37.w_0_moment2_0'], ParamOut=['linear_37.w_0']} = adam(inputs={Beta1Pow=['linear_37.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_37.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_37.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_37.w_0_moment1_0'], Moment2=['linear_37.w_0_moment2_0'], Param=['linear_37.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_38/, op_role = 2, op_role_var = ['linear_37.w_0', 'linear_37.w_0@GRAD'])
    {Out=['tmp_80']} = elementwise_mul(inputs={X=['linear_38.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_39/weight decay/, op_role = 2, op_role_var = ['linear_38.w_0', 'linear_38.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_38.w_0']} = assign(inputs={X=['tmp_80']}, op_device = , op_namescope = /optimizer_39/weight decay/, op_role = 2, op_role_var = ['linear_38.w_0', 'linear_38.w_0@GRAD'])
    {Beta1PowOut=['linear_38.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_38.w_0_beta2_pow_acc_0'], Moment1Out=['linear_38.w_0_moment1_0'], Moment2Out=['linear_38.w_0_moment2_0'], ParamOut=['linear_38.w_0']} = adam(inputs={Beta1Pow=['linear_38.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_38.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_38.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_38.w_0_moment1_0'], Moment2=['linear_38.w_0_moment2_0'], Param=['linear_38.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['linear_38.w_0', 'linear_38.w_0@GRAD'])
    {Out=['tmp_81']} = elementwise_mul(inputs={X=['linear_39.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_40/weight decay/, op_role = 2, op_role_var = ['linear_39.w_0', 'linear_39.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_39.w_0']} = assign(inputs={X=['tmp_81']}, op_device = , op_namescope = /optimizer_40/weight decay/, op_role = 2, op_role_var = ['linear_39.w_0', 'linear_39.w_0@GRAD'])
    {Beta1PowOut=['linear_39.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_39.w_0_beta2_pow_acc_0'], Moment1Out=['linear_39.w_0_moment1_0'], Moment2Out=['linear_39.w_0_moment2_0'], ParamOut=['linear_39.w_0']} = adam(inputs={Beta1Pow=['linear_39.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_39.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_39.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_39.w_0_moment1_0'], Moment2=['linear_39.w_0_moment2_0'], Param=['linear_39.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['linear_39.w_0', 'linear_39.w_0@GRAD'])
    {Out=['tmp_82']} = elementwise_mul(inputs={X=['linear_4.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_41/weight decay/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_4.w_0']} = assign(inputs={X=['tmp_82']}, op_device = , op_namescope = /optimizer_41/weight decay/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'])
    {Beta1PowOut=['linear_4.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.w_0_beta2_pow_acc_0'], Moment1Out=['linear_4.w_0_moment1_0'], Moment2Out=['linear_4.w_0_moment2_0'], ParamOut=['linear_4.w_0']} = adam(inputs={Beta1Pow=['linear_4.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_4.w_0_moment1_0'], Moment2=['linear_4.w_0_moment2_0'], Param=['linear_4.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'])
    {Out=['tmp_83']} = elementwise_mul(inputs={X=['linear_40.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_42/weight decay/, op_role = 2, op_role_var = ['linear_40.w_0', 'linear_40.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_40.w_0']} = assign(inputs={X=['tmp_83']}, op_device = , op_namescope = /optimizer_42/weight decay/, op_role = 2, op_role_var = ['linear_40.w_0', 'linear_40.w_0@GRAD'])
    {Beta1PowOut=['linear_40.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_40.w_0_beta2_pow_acc_0'], Moment1Out=['linear_40.w_0_moment1_0'], Moment2Out=['linear_40.w_0_moment2_0'], ParamOut=['linear_40.w_0']} = adam(inputs={Beta1Pow=['linear_40.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_40.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_40.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_40.w_0_moment1_0'], Moment2=['linear_40.w_0_moment2_0'], Param=['linear_40.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['linear_40.w_0', 'linear_40.w_0@GRAD'])
    {Out=['tmp_84']} = elementwise_mul(inputs={X=['linear_41.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_43/weight decay/, op_role = 2, op_role_var = ['linear_41.w_0', 'linear_41.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_41.w_0']} = assign(inputs={X=['tmp_84']}, op_device = , op_namescope = /optimizer_43/weight decay/, op_role = 2, op_role_var = ['linear_41.w_0', 'linear_41.w_0@GRAD'])
    {Beta1PowOut=['linear_41.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_41.w_0_beta2_pow_acc_0'], Moment1Out=['linear_41.w_0_moment1_0'], Moment2Out=['linear_41.w_0_moment2_0'], ParamOut=['linear_41.w_0']} = adam(inputs={Beta1Pow=['linear_41.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_41.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_41.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_41.w_0_moment1_0'], Moment2=['linear_41.w_0_moment2_0'], Param=['linear_41.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['linear_41.w_0', 'linear_41.w_0@GRAD'])
    {Out=['tmp_85']} = elementwise_mul(inputs={X=['linear_42.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_44/weight decay/, op_role = 2, op_role_var = ['linear_42.w_0', 'linear_42.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_42.w_0']} = assign(inputs={X=['tmp_85']}, op_device = , op_namescope = /optimizer_44/weight decay/, op_role = 2, op_role_var = ['linear_42.w_0', 'linear_42.w_0@GRAD'])
    {Beta1PowOut=['linear_42.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_42.w_0_beta2_pow_acc_0'], Moment1Out=['linear_42.w_0_moment1_0'], Moment2Out=['linear_42.w_0_moment2_0'], ParamOut=['linear_42.w_0']} = adam(inputs={Beta1Pow=['linear_42.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_42.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_42.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_42.w_0_moment1_0'], Moment2=['linear_42.w_0_moment2_0'], Param=['linear_42.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['linear_42.w_0', 'linear_42.w_0@GRAD'])
    {Out=['tmp_86']} = elementwise_mul(inputs={X=['linear_43.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_45/weight decay/, op_role = 2, op_role_var = ['linear_43.w_0', 'linear_43.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_43.w_0']} = assign(inputs={X=['tmp_86']}, op_device = , op_namescope = /optimizer_45/weight decay/, op_role = 2, op_role_var = ['linear_43.w_0', 'linear_43.w_0@GRAD'])
    {Beta1PowOut=['linear_43.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_43.w_0_beta2_pow_acc_0'], Moment1Out=['linear_43.w_0_moment1_0'], Moment2Out=['linear_43.w_0_moment2_0'], ParamOut=['linear_43.w_0']} = adam(inputs={Beta1Pow=['linear_43.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_43.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_43.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_43.w_0_moment1_0'], Moment2=['linear_43.w_0_moment2_0'], Param=['linear_43.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['linear_43.w_0', 'linear_43.w_0@GRAD'])
    {Out=['tmp_87']} = elementwise_mul(inputs={X=['linear_44.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_46/weight decay/, op_role = 2, op_role_var = ['linear_44.w_0', 'linear_44.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_44.w_0']} = assign(inputs={X=['tmp_87']}, op_device = , op_namescope = /optimizer_46/weight decay/, op_role = 2, op_role_var = ['linear_44.w_0', 'linear_44.w_0@GRAD'])
    {Beta1PowOut=['linear_44.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_44.w_0_beta2_pow_acc_0'], Moment1Out=['linear_44.w_0_moment1_0'], Moment2Out=['linear_44.w_0_moment2_0'], ParamOut=['linear_44.w_0']} = adam(inputs={Beta1Pow=['linear_44.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_44.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_44.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_44.w_0_moment1_0'], Moment2=['linear_44.w_0_moment2_0'], Param=['linear_44.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['linear_44.w_0', 'linear_44.w_0@GRAD'])
    {Out=['tmp_88']} = elementwise_mul(inputs={X=['linear_45.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_47/weight decay/, op_role = 2, op_role_var = ['linear_45.w_0', 'linear_45.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_45.w_0']} = assign(inputs={X=['tmp_88']}, op_device = , op_namescope = /optimizer_47/weight decay/, op_role = 2, op_role_var = ['linear_45.w_0', 'linear_45.w_0@GRAD'])
    {Beta1PowOut=['linear_45.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_45.w_0_beta2_pow_acc_0'], Moment1Out=['linear_45.w_0_moment1_0'], Moment2Out=['linear_45.w_0_moment2_0'], ParamOut=['linear_45.w_0']} = adam(inputs={Beta1Pow=['linear_45.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_45.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_45.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_45.w_0_moment1_0'], Moment2=['linear_45.w_0_moment2_0'], Param=['linear_45.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['linear_45.w_0', 'linear_45.w_0@GRAD'])
    {Out=['tmp_89']} = elementwise_mul(inputs={X=['linear_46.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_48/weight decay/, op_role = 2, op_role_var = ['linear_46.w_0', 'linear_46.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_46.w_0']} = assign(inputs={X=['tmp_89']}, op_device = , op_namescope = /optimizer_48/weight decay/, op_role = 2, op_role_var = ['linear_46.w_0', 'linear_46.w_0@GRAD'])
    {Beta1PowOut=['linear_46.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_46.w_0_beta2_pow_acc_0'], Moment1Out=['linear_46.w_0_moment1_0'], Moment2Out=['linear_46.w_0_moment2_0'], ParamOut=['linear_46.w_0']} = adam(inputs={Beta1Pow=['linear_46.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_46.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_46.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_46.w_0_moment1_0'], Moment2=['linear_46.w_0_moment2_0'], Param=['linear_46.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['linear_46.w_0', 'linear_46.w_0@GRAD'])
    {Out=['tmp_90']} = elementwise_mul(inputs={X=['linear_47.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_49/weight decay/, op_role = 2, op_role_var = ['linear_47.w_0', 'linear_47.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_47.w_0']} = assign(inputs={X=['tmp_90']}, op_device = , op_namescope = /optimizer_49/weight decay/, op_role = 2, op_role_var = ['linear_47.w_0', 'linear_47.w_0@GRAD'])
    {Beta1PowOut=['linear_47.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_47.w_0_beta2_pow_acc_0'], Moment1Out=['linear_47.w_0_moment1_0'], Moment2Out=['linear_47.w_0_moment2_0'], ParamOut=['linear_47.w_0']} = adam(inputs={Beta1Pow=['linear_47.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_47.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_47.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_47.w_0_moment1_0'], Moment2=['linear_47.w_0_moment2_0'], Param=['linear_47.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['linear_47.w_0', 'linear_47.w_0@GRAD'])
    {Out=['tmp_91']} = elementwise_mul(inputs={X=['linear_48.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_50/weight decay/, op_role = 2, op_role_var = ['linear_48.w_0', 'linear_48.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_48.w_0']} = assign(inputs={X=['tmp_91']}, op_device = , op_namescope = /optimizer_50/weight decay/, op_role = 2, op_role_var = ['linear_48.w_0', 'linear_48.w_0@GRAD'])
    {Beta1PowOut=['linear_48.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_48.w_0_beta2_pow_acc_0'], Moment1Out=['linear_48.w_0_moment1_0'], Moment2Out=['linear_48.w_0_moment2_0'], ParamOut=['linear_48.w_0']} = adam(inputs={Beta1Pow=['linear_48.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_48.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_48.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_48.w_0_moment1_0'], Moment2=['linear_48.w_0_moment2_0'], Param=['linear_48.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['linear_48.w_0', 'linear_48.w_0@GRAD'])
    {Out=['tmp_92']} = elementwise_mul(inputs={X=['linear_49.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_51/weight decay/, op_role = 2, op_role_var = ['linear_49.w_0', 'linear_49.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_49.w_0']} = assign(inputs={X=['tmp_92']}, op_device = , op_namescope = /optimizer_51/weight decay/, op_role = 2, op_role_var = ['linear_49.w_0', 'linear_49.w_0@GRAD'])
    {Beta1PowOut=['linear_49.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_49.w_0_beta2_pow_acc_0'], Moment1Out=['linear_49.w_0_moment1_0'], Moment2Out=['linear_49.w_0_moment2_0'], ParamOut=['linear_49.w_0']} = adam(inputs={Beta1Pow=['linear_49.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_49.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_49.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_49.w_0_moment1_0'], Moment2=['linear_49.w_0_moment2_0'], Param=['linear_49.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['linear_49.w_0', 'linear_49.w_0@GRAD'])
    {Out=['tmp_93']} = elementwise_mul(inputs={X=['linear_5.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_52/weight decay/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_5.w_0']} = assign(inputs={X=['tmp_93']}, op_device = , op_namescope = /optimizer_52/weight decay/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'])
    {Beta1PowOut=['linear_5.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.w_0_beta2_pow_acc_0'], Moment1Out=['linear_5.w_0_moment1_0'], Moment2Out=['linear_5.w_0_moment2_0'], ParamOut=['linear_5.w_0']} = adam(inputs={Beta1Pow=['linear_5.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_5.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_5.w_0_moment1_0'], Moment2=['linear_5.w_0_moment2_0'], Param=['linear_5.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'])
    {Out=['tmp_94']} = elementwise_mul(inputs={X=['linear_50.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_53/weight decay/, op_role = 2, op_role_var = ['linear_50.w_0', 'linear_50.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_50.w_0']} = assign(inputs={X=['tmp_94']}, op_device = , op_namescope = /optimizer_53/weight decay/, op_role = 2, op_role_var = ['linear_50.w_0', 'linear_50.w_0@GRAD'])
    {Beta1PowOut=['linear_50.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_50.w_0_beta2_pow_acc_0'], Moment1Out=['linear_50.w_0_moment1_0'], Moment2Out=['linear_50.w_0_moment2_0'], ParamOut=['linear_50.w_0']} = adam(inputs={Beta1Pow=['linear_50.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_50.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_50.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_50.w_0_moment1_0'], Moment2=['linear_50.w_0_moment2_0'], Param=['linear_50.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['linear_50.w_0', 'linear_50.w_0@GRAD'])
    {Out=['tmp_95']} = elementwise_mul(inputs={X=['linear_51.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_54/weight decay/, op_role = 2, op_role_var = ['linear_51.w_0', 'linear_51.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_51.w_0']} = assign(inputs={X=['tmp_95']}, op_device = , op_namescope = /optimizer_54/weight decay/, op_role = 2, op_role_var = ['linear_51.w_0', 'linear_51.w_0@GRAD'])
    {Beta1PowOut=['linear_51.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_51.w_0_beta2_pow_acc_0'], Moment1Out=['linear_51.w_0_moment1_0'], Moment2Out=['linear_51.w_0_moment2_0'], ParamOut=['linear_51.w_0']} = adam(inputs={Beta1Pow=['linear_51.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_51.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_51.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_51.w_0_moment1_0'], Moment2=['linear_51.w_0_moment2_0'], Param=['linear_51.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['linear_51.w_0', 'linear_51.w_0@GRAD'])
    {Out=['tmp_96']} = elementwise_mul(inputs={X=['linear_52.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_55/weight decay/, op_role = 2, op_role_var = ['linear_52.w_0', 'linear_52.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_52.w_0']} = assign(inputs={X=['tmp_96']}, op_device = , op_namescope = /optimizer_55/weight decay/, op_role = 2, op_role_var = ['linear_52.w_0', 'linear_52.w_0@GRAD'])
    {Beta1PowOut=['linear_52.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_52.w_0_beta2_pow_acc_0'], Moment1Out=['linear_52.w_0_moment1_0'], Moment2Out=['linear_52.w_0_moment2_0'], ParamOut=['linear_52.w_0']} = adam(inputs={Beta1Pow=['linear_52.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_52.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_52.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_52.w_0_moment1_0'], Moment2=['linear_52.w_0_moment2_0'], Param=['linear_52.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['linear_52.w_0', 'linear_52.w_0@GRAD'])
    {Out=['tmp_97']} = elementwise_mul(inputs={X=['linear_53.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_56/weight decay/, op_role = 2, op_role_var = ['linear_53.w_0', 'linear_53.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_53.w_0']} = assign(inputs={X=['tmp_97']}, op_device = , op_namescope = /optimizer_56/weight decay/, op_role = 2, op_role_var = ['linear_53.w_0', 'linear_53.w_0@GRAD'])
    {Beta1PowOut=['linear_53.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_53.w_0_beta2_pow_acc_0'], Moment1Out=['linear_53.w_0_moment1_0'], Moment2Out=['linear_53.w_0_moment2_0'], ParamOut=['linear_53.w_0']} = adam(inputs={Beta1Pow=['linear_53.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_53.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_53.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_53.w_0_moment1_0'], Moment2=['linear_53.w_0_moment2_0'], Param=['linear_53.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['linear_53.w_0', 'linear_53.w_0@GRAD'])
    {Out=['tmp_98']} = elementwise_mul(inputs={X=['linear_54.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_57/weight decay/, op_role = 2, op_role_var = ['linear_54.w_0', 'linear_54.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_54.w_0']} = assign(inputs={X=['tmp_98']}, op_device = , op_namescope = /optimizer_57/weight decay/, op_role = 2, op_role_var = ['linear_54.w_0', 'linear_54.w_0@GRAD'])
    {Beta1PowOut=['linear_54.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_54.w_0_beta2_pow_acc_0'], Moment1Out=['linear_54.w_0_moment1_0'], Moment2Out=['linear_54.w_0_moment2_0'], ParamOut=['linear_54.w_0']} = adam(inputs={Beta1Pow=['linear_54.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_54.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_54.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_54.w_0_moment1_0'], Moment2=['linear_54.w_0_moment2_0'], Param=['linear_54.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['linear_54.w_0', 'linear_54.w_0@GRAD'])
    {Out=['tmp_99']} = elementwise_mul(inputs={X=['linear_55.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_58/weight decay/, op_role = 2, op_role_var = ['linear_55.w_0', 'linear_55.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_55.w_0']} = assign(inputs={X=['tmp_99']}, op_device = , op_namescope = /optimizer_58/weight decay/, op_role = 2, op_role_var = ['linear_55.w_0', 'linear_55.w_0@GRAD'])
    {Beta1PowOut=['linear_55.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_55.w_0_beta2_pow_acc_0'], Moment1Out=['linear_55.w_0_moment1_0'], Moment2Out=['linear_55.w_0_moment2_0'], ParamOut=['linear_55.w_0']} = adam(inputs={Beta1Pow=['linear_55.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_55.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_55.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_55.w_0_moment1_0'], Moment2=['linear_55.w_0_moment2_0'], Param=['linear_55.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_58/, op_role = 2, op_role_var = ['linear_55.w_0', 'linear_55.w_0@GRAD'])
    {Out=['tmp_100']} = elementwise_mul(inputs={X=['linear_56.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_59/weight decay/, op_role = 2, op_role_var = ['linear_56.w_0', 'linear_56.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_56.w_0']} = assign(inputs={X=['tmp_100']}, op_device = , op_namescope = /optimizer_59/weight decay/, op_role = 2, op_role_var = ['linear_56.w_0', 'linear_56.w_0@GRAD'])
    {Beta1PowOut=['linear_56.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_56.w_0_beta2_pow_acc_0'], Moment1Out=['linear_56.w_0_moment1_0'], Moment2Out=['linear_56.w_0_moment2_0'], ParamOut=['linear_56.w_0']} = adam(inputs={Beta1Pow=['linear_56.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_56.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_56.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_56.w_0_moment1_0'], Moment2=['linear_56.w_0_moment2_0'], Param=['linear_56.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_59/, op_role = 2, op_role_var = ['linear_56.w_0', 'linear_56.w_0@GRAD'])
    {Out=['tmp_101']} = elementwise_mul(inputs={X=['linear_57.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_60/weight decay/, op_role = 2, op_role_var = ['linear_57.w_0', 'linear_57.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_57.w_0']} = assign(inputs={X=['tmp_101']}, op_device = , op_namescope = /optimizer_60/weight decay/, op_role = 2, op_role_var = ['linear_57.w_0', 'linear_57.w_0@GRAD'])
    {Beta1PowOut=['linear_57.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_57.w_0_beta2_pow_acc_0'], Moment1Out=['linear_57.w_0_moment1_0'], Moment2Out=['linear_57.w_0_moment2_0'], ParamOut=['linear_57.w_0']} = adam(inputs={Beta1Pow=['linear_57.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_57.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_57.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_57.w_0_moment1_0'], Moment2=['linear_57.w_0_moment2_0'], Param=['linear_57.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_60/, op_role = 2, op_role_var = ['linear_57.w_0', 'linear_57.w_0@GRAD'])
    {Out=['tmp_102']} = elementwise_mul(inputs={X=['linear_58.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_61/weight decay/, op_role = 2, op_role_var = ['linear_58.w_0', 'linear_58.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_58.w_0']} = assign(inputs={X=['tmp_102']}, op_device = , op_namescope = /optimizer_61/weight decay/, op_role = 2, op_role_var = ['linear_58.w_0', 'linear_58.w_0@GRAD'])
    {Beta1PowOut=['linear_58.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_58.w_0_beta2_pow_acc_0'], Moment1Out=['linear_58.w_0_moment1_0'], Moment2Out=['linear_58.w_0_moment2_0'], ParamOut=['linear_58.w_0']} = adam(inputs={Beta1Pow=['linear_58.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_58.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_58.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_58.w_0_moment1_0'], Moment2=['linear_58.w_0_moment2_0'], Param=['linear_58.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_61/, op_role = 2, op_role_var = ['linear_58.w_0', 'linear_58.w_0@GRAD'])
    {Out=['tmp_103']} = elementwise_mul(inputs={X=['linear_59.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_62/weight decay/, op_role = 2, op_role_var = ['linear_59.w_0', 'linear_59.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_59.w_0']} = assign(inputs={X=['tmp_103']}, op_device = , op_namescope = /optimizer_62/weight decay/, op_role = 2, op_role_var = ['linear_59.w_0', 'linear_59.w_0@GRAD'])
    {Beta1PowOut=['linear_59.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_59.w_0_beta2_pow_acc_0'], Moment1Out=['linear_59.w_0_moment1_0'], Moment2Out=['linear_59.w_0_moment2_0'], ParamOut=['linear_59.w_0']} = adam(inputs={Beta1Pow=['linear_59.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_59.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_59.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_59.w_0_moment1_0'], Moment2=['linear_59.w_0_moment2_0'], Param=['linear_59.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_62/, op_role = 2, op_role_var = ['linear_59.w_0', 'linear_59.w_0@GRAD'])
    {Out=['tmp_104']} = elementwise_mul(inputs={X=['linear_6.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_63/weight decay/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_6.w_0']} = assign(inputs={X=['tmp_104']}, op_device = , op_namescope = /optimizer_63/weight decay/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'])
    {Beta1PowOut=['linear_6.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.w_0_beta2_pow_acc_0'], Moment1Out=['linear_6.w_0_moment1_0'], Moment2Out=['linear_6.w_0_moment2_0'], ParamOut=['linear_6.w_0']} = adam(inputs={Beta1Pow=['linear_6.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_6.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_6.w_0_moment1_0'], Moment2=['linear_6.w_0_moment2_0'], Param=['linear_6.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_63/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'])
    {Out=['tmp_105']} = elementwise_mul(inputs={X=['linear_60.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_64/weight decay/, op_role = 2, op_role_var = ['linear_60.w_0', 'linear_60.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_60.w_0']} = assign(inputs={X=['tmp_105']}, op_device = , op_namescope = /optimizer_64/weight decay/, op_role = 2, op_role_var = ['linear_60.w_0', 'linear_60.w_0@GRAD'])
    {Beta1PowOut=['linear_60.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_60.w_0_beta2_pow_acc_0'], Moment1Out=['linear_60.w_0_moment1_0'], Moment2Out=['linear_60.w_0_moment2_0'], ParamOut=['linear_60.w_0']} = adam(inputs={Beta1Pow=['linear_60.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_60.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_60.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_60.w_0_moment1_0'], Moment2=['linear_60.w_0_moment2_0'], Param=['linear_60.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_64/, op_role = 2, op_role_var = ['linear_60.w_0', 'linear_60.w_0@GRAD'])
    {Out=['tmp_106']} = elementwise_mul(inputs={X=['linear_61.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_65/weight decay/, op_role = 2, op_role_var = ['linear_61.w_0', 'linear_61.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_61.w_0']} = assign(inputs={X=['tmp_106']}, op_device = , op_namescope = /optimizer_65/weight decay/, op_role = 2, op_role_var = ['linear_61.w_0', 'linear_61.w_0@GRAD'])
    {Beta1PowOut=['linear_61.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_61.w_0_beta2_pow_acc_0'], Moment1Out=['linear_61.w_0_moment1_0'], Moment2Out=['linear_61.w_0_moment2_0'], ParamOut=['linear_61.w_0']} = adam(inputs={Beta1Pow=['linear_61.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_61.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_61.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_61.w_0_moment1_0'], Moment2=['linear_61.w_0_moment2_0'], Param=['linear_61.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_65/, op_role = 2, op_role_var = ['linear_61.w_0', 'linear_61.w_0@GRAD'])
    {Out=['tmp_107']} = elementwise_mul(inputs={X=['linear_62.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_66/weight decay/, op_role = 2, op_role_var = ['linear_62.w_0', 'linear_62.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_62.w_0']} = assign(inputs={X=['tmp_107']}, op_device = , op_namescope = /optimizer_66/weight decay/, op_role = 2, op_role_var = ['linear_62.w_0', 'linear_62.w_0@GRAD'])
    {Beta1PowOut=['linear_62.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_62.w_0_beta2_pow_acc_0'], Moment1Out=['linear_62.w_0_moment1_0'], Moment2Out=['linear_62.w_0_moment2_0'], ParamOut=['linear_62.w_0']} = adam(inputs={Beta1Pow=['linear_62.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_62.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_62.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_62.w_0_moment1_0'], Moment2=['linear_62.w_0_moment2_0'], Param=['linear_62.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_66/, op_role = 2, op_role_var = ['linear_62.w_0', 'linear_62.w_0@GRAD'])
    {Out=['tmp_108']} = elementwise_mul(inputs={X=['linear_63.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_67/weight decay/, op_role = 2, op_role_var = ['linear_63.w_0', 'linear_63.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_63.w_0']} = assign(inputs={X=['tmp_108']}, op_device = , op_namescope = /optimizer_67/weight decay/, op_role = 2, op_role_var = ['linear_63.w_0', 'linear_63.w_0@GRAD'])
    {Beta1PowOut=['linear_63.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_63.w_0_beta2_pow_acc_0'], Moment1Out=['linear_63.w_0_moment1_0'], Moment2Out=['linear_63.w_0_moment2_0'], ParamOut=['linear_63.w_0']} = adam(inputs={Beta1Pow=['linear_63.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_63.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_63.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_63.w_0_moment1_0'], Moment2=['linear_63.w_0_moment2_0'], Param=['linear_63.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_67/, op_role = 2, op_role_var = ['linear_63.w_0', 'linear_63.w_0@GRAD'])
    {Out=['tmp_109']} = elementwise_mul(inputs={X=['linear_64.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_68/weight decay/, op_role = 2, op_role_var = ['linear_64.w_0', 'linear_64.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_64.w_0']} = assign(inputs={X=['tmp_109']}, op_device = , op_namescope = /optimizer_68/weight decay/, op_role = 2, op_role_var = ['linear_64.w_0', 'linear_64.w_0@GRAD'])
    {Beta1PowOut=['linear_64.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_64.w_0_beta2_pow_acc_0'], Moment1Out=['linear_64.w_0_moment1_0'], Moment2Out=['linear_64.w_0_moment2_0'], ParamOut=['linear_64.w_0']} = adam(inputs={Beta1Pow=['linear_64.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_64.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_64.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_64.w_0_moment1_0'], Moment2=['linear_64.w_0_moment2_0'], Param=['linear_64.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_68/, op_role = 2, op_role_var = ['linear_64.w_0', 'linear_64.w_0@GRAD'])
    {Out=['tmp_110']} = elementwise_mul(inputs={X=['linear_65.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_69/weight decay/, op_role = 2, op_role_var = ['linear_65.w_0', 'linear_65.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_65.w_0']} = assign(inputs={X=['tmp_110']}, op_device = , op_namescope = /optimizer_69/weight decay/, op_role = 2, op_role_var = ['linear_65.w_0', 'linear_65.w_0@GRAD'])
    {Beta1PowOut=['linear_65.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_65.w_0_beta2_pow_acc_0'], Moment1Out=['linear_65.w_0_moment1_0'], Moment2Out=['linear_65.w_0_moment2_0'], ParamOut=['linear_65.w_0']} = adam(inputs={Beta1Pow=['linear_65.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_65.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_65.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_65.w_0_moment1_0'], Moment2=['linear_65.w_0_moment2_0'], Param=['linear_65.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_69/, op_role = 2, op_role_var = ['linear_65.w_0', 'linear_65.w_0@GRAD'])
    {Out=['tmp_111']} = elementwise_mul(inputs={X=['linear_66.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_70/weight decay/, op_role = 2, op_role_var = ['linear_66.w_0', 'linear_66.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_66.w_0']} = assign(inputs={X=['tmp_111']}, op_device = , op_namescope = /optimizer_70/weight decay/, op_role = 2, op_role_var = ['linear_66.w_0', 'linear_66.w_0@GRAD'])
    {Beta1PowOut=['linear_66.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_66.w_0_beta2_pow_acc_0'], Moment1Out=['linear_66.w_0_moment1_0'], Moment2Out=['linear_66.w_0_moment2_0'], ParamOut=['linear_66.w_0']} = adam(inputs={Beta1Pow=['linear_66.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_66.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_66.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_66.w_0_moment1_0'], Moment2=['linear_66.w_0_moment2_0'], Param=['linear_66.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_70/, op_role = 2, op_role_var = ['linear_66.w_0', 'linear_66.w_0@GRAD'])
    {Out=['tmp_112']} = elementwise_mul(inputs={X=['linear_67.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_71/weight decay/, op_role = 2, op_role_var = ['linear_67.w_0', 'linear_67.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_67.w_0']} = assign(inputs={X=['tmp_112']}, op_device = , op_namescope = /optimizer_71/weight decay/, op_role = 2, op_role_var = ['linear_67.w_0', 'linear_67.w_0@GRAD'])
    {Beta1PowOut=['linear_67.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_67.w_0_beta2_pow_acc_0'], Moment1Out=['linear_67.w_0_moment1_0'], Moment2Out=['linear_67.w_0_moment2_0'], ParamOut=['linear_67.w_0']} = adam(inputs={Beta1Pow=['linear_67.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_67.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_67.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_67.w_0_moment1_0'], Moment2=['linear_67.w_0_moment2_0'], Param=['linear_67.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_71/, op_role = 2, op_role_var = ['linear_67.w_0', 'linear_67.w_0@GRAD'])
    {Out=['tmp_113']} = elementwise_mul(inputs={X=['linear_68.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_72/weight decay/, op_role = 2, op_role_var = ['linear_68.w_0', 'linear_68.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_68.w_0']} = assign(inputs={X=['tmp_113']}, op_device = , op_namescope = /optimizer_72/weight decay/, op_role = 2, op_role_var = ['linear_68.w_0', 'linear_68.w_0@GRAD'])
    {Beta1PowOut=['linear_68.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_68.w_0_beta2_pow_acc_0'], Moment1Out=['linear_68.w_0_moment1_0'], Moment2Out=['linear_68.w_0_moment2_0'], ParamOut=['linear_68.w_0']} = adam(inputs={Beta1Pow=['linear_68.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_68.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_68.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_68.w_0_moment1_0'], Moment2=['linear_68.w_0_moment2_0'], Param=['linear_68.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_72/, op_role = 2, op_role_var = ['linear_68.w_0', 'linear_68.w_0@GRAD'])
    {Out=['tmp_114']} = elementwise_mul(inputs={X=['linear_69.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_73/weight decay/, op_role = 2, op_role_var = ['linear_69.w_0', 'linear_69.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_69.w_0']} = assign(inputs={X=['tmp_114']}, op_device = , op_namescope = /optimizer_73/weight decay/, op_role = 2, op_role_var = ['linear_69.w_0', 'linear_69.w_0@GRAD'])
    {Beta1PowOut=['linear_69.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_69.w_0_beta2_pow_acc_0'], Moment1Out=['linear_69.w_0_moment1_0'], Moment2Out=['linear_69.w_0_moment2_0'], ParamOut=['linear_69.w_0']} = adam(inputs={Beta1Pow=['linear_69.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_69.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_69.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_69.w_0_moment1_0'], Moment2=['linear_69.w_0_moment2_0'], Param=['linear_69.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_73/, op_role = 2, op_role_var = ['linear_69.w_0', 'linear_69.w_0@GRAD'])
    {Out=['tmp_115']} = elementwise_mul(inputs={X=['linear_7.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_74/weight decay/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_7.w_0']} = assign(inputs={X=['tmp_115']}, op_device = , op_namescope = /optimizer_74/weight decay/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'])
    {Beta1PowOut=['linear_7.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.w_0_beta2_pow_acc_0'], Moment1Out=['linear_7.w_0_moment1_0'], Moment2Out=['linear_7.w_0_moment2_0'], ParamOut=['linear_7.w_0']} = adam(inputs={Beta1Pow=['linear_7.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_7.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_7.w_0_moment1_0'], Moment2=['linear_7.w_0_moment2_0'], Param=['linear_7.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_74/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'])
    {Out=['tmp_116']} = elementwise_mul(inputs={X=['linear_70.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_75/weight decay/, op_role = 2, op_role_var = ['linear_70.w_0', 'linear_70.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_70.w_0']} = assign(inputs={X=['tmp_116']}, op_device = , op_namescope = /optimizer_75/weight decay/, op_role = 2, op_role_var = ['linear_70.w_0', 'linear_70.w_0@GRAD'])
    {Beta1PowOut=['linear_70.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_70.w_0_beta2_pow_acc_0'], Moment1Out=['linear_70.w_0_moment1_0'], Moment2Out=['linear_70.w_0_moment2_0'], ParamOut=['linear_70.w_0']} = adam(inputs={Beta1Pow=['linear_70.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_70.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_70.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_70.w_0_moment1_0'], Moment2=['linear_70.w_0_moment2_0'], Param=['linear_70.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_75/, op_role = 2, op_role_var = ['linear_70.w_0', 'linear_70.w_0@GRAD'])
    {Out=['tmp_117']} = elementwise_mul(inputs={X=['linear_71.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_76/weight decay/, op_role = 2, op_role_var = ['linear_71.w_0', 'linear_71.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_71.w_0']} = assign(inputs={X=['tmp_117']}, op_device = , op_namescope = /optimizer_76/weight decay/, op_role = 2, op_role_var = ['linear_71.w_0', 'linear_71.w_0@GRAD'])
    {Beta1PowOut=['linear_71.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_71.w_0_beta2_pow_acc_0'], Moment1Out=['linear_71.w_0_moment1_0'], Moment2Out=['linear_71.w_0_moment2_0'], ParamOut=['linear_71.w_0']} = adam(inputs={Beta1Pow=['linear_71.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_71.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_71.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_71.w_0_moment1_0'], Moment2=['linear_71.w_0_moment2_0'], Param=['linear_71.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_76/, op_role = 2, op_role_var = ['linear_71.w_0', 'linear_71.w_0@GRAD'])
    {Beta1PowOut=['linear_72.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_72.b_0_beta2_pow_acc_0'], Moment1Out=['linear_72.b_0_moment1_0'], Moment2Out=['linear_72.b_0_moment2_0'], ParamOut=['linear_72.b_0']} = adam(inputs={Beta1Pow=['linear_72.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_72.b_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_72.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_72.b_0_moment1_0'], Moment2=['linear_72.b_0_moment2_0'], Param=['linear_72.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_77/, op_role = 2, op_role_var = ['linear_72.b_0', 'linear_72.b_0@GRAD'])
    {Out=['tmp_118']} = elementwise_mul(inputs={X=['linear_72.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_78/weight decay/, op_role = 2, op_role_var = ['linear_72.w_0', 'linear_72.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_72.w_0']} = assign(inputs={X=['tmp_118']}, op_device = , op_namescope = /optimizer_78/weight decay/, op_role = 2, op_role_var = ['linear_72.w_0', 'linear_72.w_0@GRAD'])
    {Beta1PowOut=['linear_72.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_72.w_0_beta2_pow_acc_0'], Moment1Out=['linear_72.w_0_moment1_0'], Moment2Out=['linear_72.w_0_moment2_0'], ParamOut=['linear_72.w_0']} = adam(inputs={Beta1Pow=['linear_72.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_72.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_72.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_72.w_0_moment1_0'], Moment2=['linear_72.w_0_moment2_0'], Param=['linear_72.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_78/, op_role = 2, op_role_var = ['linear_72.w_0', 'linear_72.w_0@GRAD'])
    {Out=['tmp_119']} = elementwise_mul(inputs={X=['linear_73.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_79/weight decay/, op_role = 2, op_role_var = ['linear_73.w_0', 'linear_73.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_73.w_0']} = assign(inputs={X=['tmp_119']}, op_device = , op_namescope = /optimizer_79/weight decay/, op_role = 2, op_role_var = ['linear_73.w_0', 'linear_73.w_0@GRAD'])
    {Beta1PowOut=['linear_73.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_73.w_0_beta2_pow_acc_0'], Moment1Out=['linear_73.w_0_moment1_0'], Moment2Out=['linear_73.w_0_moment2_0'], ParamOut=['linear_73.w_0']} = adam(inputs={Beta1Pow=['linear_73.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_73.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_73.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_73.w_0_moment1_0'], Moment2=['linear_73.w_0_moment2_0'], Param=['linear_73.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_79/, op_role = 2, op_role_var = ['linear_73.w_0', 'linear_73.w_0@GRAD'])
    {Out=['tmp_120']} = elementwise_mul(inputs={X=['linear_74.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_80/weight decay/, op_role = 2, op_role_var = ['linear_74.w_0', 'linear_74.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_74.w_0']} = assign(inputs={X=['tmp_120']}, op_device = , op_namescope = /optimizer_80/weight decay/, op_role = 2, op_role_var = ['linear_74.w_0', 'linear_74.w_0@GRAD'])
    {Beta1PowOut=['linear_74.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_74.w_0_beta2_pow_acc_0'], Moment1Out=['linear_74.w_0_moment1_0'], Moment2Out=['linear_74.w_0_moment2_0'], ParamOut=['linear_74.w_0']} = adam(inputs={Beta1Pow=['linear_74.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_74.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_74.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_74.w_0_moment1_0'], Moment2=['linear_74.w_0_moment2_0'], Param=['linear_74.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_80/, op_role = 2, op_role_var = ['linear_74.w_0', 'linear_74.w_0@GRAD'])
    {Out=['tmp_121']} = elementwise_mul(inputs={X=['linear_8.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_81/weight decay/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_8.w_0']} = assign(inputs={X=['tmp_121']}, op_device = , op_namescope = /optimizer_81/weight decay/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'])
    {Beta1PowOut=['linear_8.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.w_0_beta2_pow_acc_0'], Moment1Out=['linear_8.w_0_moment1_0'], Moment2Out=['linear_8.w_0_moment2_0'], ParamOut=['linear_8.w_0']} = adam(inputs={Beta1Pow=['linear_8.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_8.w_0_moment1_0'], Moment2=['linear_8.w_0_moment2_0'], Param=['linear_8.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_81/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'])
    {Out=['tmp_122']} = elementwise_mul(inputs={X=['linear_9.w_0'], Y=['tmp_43']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /optimizer_82/weight decay/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['linear_9.w_0']} = assign(inputs={X=['tmp_122']}, op_device = , op_namescope = /optimizer_82/weight decay/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'])
    {Beta1PowOut=['linear_9.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.w_0_beta2_pow_acc_0'], Moment1Out=['linear_9.w_0_moment1_0'], Moment2Out=['linear_9.w_0_moment2_0'], ParamOut=['linear_9.w_0']} = adam(inputs={Beta1Pow=['linear_9.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.w_0_beta2_pow_acc_0'], Beta2Tensor=[], Grad=['linear_9.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_9.w_0_moment1_0'], Moment2=['linear_9.w_0_moment2_0'], Param=['linear_9.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.999999974752427e-07, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_82/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'])
}
